{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e15367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON loaded, items: 5047\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_or_create_data(json_path, excel_path, labels_path, video_dir, text_dir, caption_dir):\n",
    "    \"\"\"\n",
    "    Load labels from NPY and JSON. If JSON is missing or mismatched, generate it from Excel.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): path to the JSON file storing data_list\n",
    "        excel_path (str): path to the Excel file containing the video list\n",
    "        labels_path (str): path to the NPY file containing labels\n",
    "        video_dir (str): directory containing .npy video features\n",
    "        text_dir (str): directory containing .npy text features\n",
    "        caption_dir (str): directory containing .npy caption features\n",
    "\n",
    "    Returns:\n",
    "        data_list (list of dict): list of dictionaries with keys 'video_feat', 'text_feat', 'caption_feat'\n",
    "        labels (np.ndarray): labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Load labels\n",
    "    if not os.path.exists(labels_path):\n",
    "        raise FileNotFoundError(f\"Labels file not found: {labels_path}\")\n",
    "    labels = np.load(labels_path)\n",
    "\n",
    "    # Check JSON\n",
    "    need_create_json = True\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data_list = json.load(f)\n",
    "                if len(data_list) == len(labels):\n",
    "                    need_create_json = False\n",
    "            except json.JSONDecodeError:\n",
    "                need_create_json = True\n",
    "\n",
    "    # Create JSON \n",
    "    if need_create_json:\n",
    "        if not os.path.exists(excel_path):\n",
    "            raise FileNotFoundError(f\"Excel file not found: {excel_path}\")\n",
    "        df = pd.read_excel(excel_path)\n",
    "        data_list = []\n",
    "        for vid in df['video_id'].astype(str):\n",
    "            data_list.append({\n",
    "                \"name\": vid,\n",
    "                \"video_feat\": os.path.join(video_dir, f\"{vid}.npy\"),\n",
    "                \"text_feat\": os.path.join(text_dir, f\"{vid}.npy\"),\n",
    "                \"caption_feat\": os.path.join(caption_dir, f\"{vid}.npy\")\n",
    "            })\n",
    "        # Create folder\n",
    "        os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "        # Save JSON\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data_list, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"JSON created, items: {len(data_list)}\")\n",
    "    else:\n",
    "        print(f\"JSON loaded, items: {len(data_list)}\")\n",
    "\n",
    "    return data_list, labels\n",
    "\n",
    "video_dir = \"/Users/jumita/Downloads/final/frame\"\n",
    "text_dir = \"/Users/jumita/Downloads/final/text\"\n",
    "caption_dir = \"/Users/jumita/Downloads/final/caption\"\n",
    "json_path = \"/Users/jumita/Downloads/final_code/data.json\"\n",
    "excel_path = \"/Users/jumita/Downloads/Book5.xlsx\"\n",
    "labels_path = \"/Users/jumita/Downloads/final_code/labels.npy\"\n",
    "\n",
    "data_list, labels = load_or_create_data(\n",
    "    json_path=json_path,\n",
    "    excel_path=excel_path,\n",
    "    labels_path=labels_path,\n",
    "    video_dir=video_dir,\n",
    "    text_dir=text_dir,\n",
    "    caption_dir=caption_dir\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520ab72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON loaded, items: 5047\n",
      "Overlap train-val: 0\n",
      "Overlap train-test: 0\n",
      "Overlap val-test: 0\n",
      "Augmented sample shapes:\n",
      "Video: torch.Size([16, 256])\n",
      "Text: torch.Size([16, 256])\n",
      "Caption: torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ================= VIDEO FEATURE AUGMENTER =================\n",
    "class VideoFeatureAugmenter:\n",
    "    \"\"\"\n",
    "    Data augmenter for video features.\n",
    "\n",
    "    This class applies temporal cropping and adaptive Gaussian noise \n",
    "    to video features to improve model generalization during training.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    augment_prob : float\n",
    "        Probability of applying augmentation.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer used for augmentation (optional, not used here).\n",
    "    noise_scale : float\n",
    "        Scaling factor for Gaussian noise.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    temporal_crop(features, crop_ratio=0.9):\n",
    "        Randomly crops the sequence along the temporal dimension.\n",
    "    adaptive_noise(features):\n",
    "        Adds Gaussian noise proportional to the feature's standard deviation.\n",
    "    __call__(features):\n",
    "        Applies augmentation with probability `augment_prob`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, augment_prob=0.5, dropout_prob=0.3, noise_scale=0.05):\n",
    "        self.augment_prob = augment_prob\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "    def temporal_crop(self, features, crop_ratio=0.9):\n",
    "        \"\"\"\n",
    "        Crop the feature sequence randomly along the temporal dimension.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Video feature tensor of shape (T, D) or (T, H, D).\n",
    "        crop_ratio : float\n",
    "            Fraction of the sequence to keep.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Cropped feature tensor.\n",
    "        \"\"\"\n",
    "        T = features.shape[0]\n",
    "        new_T = int(T * crop_ratio)\n",
    "        start = np.random.randint(0, T - new_T + 1)\n",
    "        return features[start:start+new_T]\n",
    "\n",
    "    def adaptive_noise(self, features):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise scaled by feature standard deviation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Noisy feature tensor.\n",
    "        \"\"\"\n",
    "        std_per_feature = features.std(dim=0, keepdim=True) + 1e-6\n",
    "        noise = torch.randn_like(features) * std_per_feature * self.noise_scale\n",
    "        return features + noise\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Apply augmentation with probability `augment_prob`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Input feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Augmented feature tensor.\n",
    "        \"\"\"\n",
    "        if torch.rand(1) < self.augment_prob:\n",
    "            if features.ndim >= 2:\n",
    "                features = self.temporal_crop(features)\n",
    "            features = self.adaptive_noise(features)\n",
    "        return features\n",
    "\n",
    "# ================= TEXT/CAPTION FEATURE AUGMENTER =================\n",
    "class TextFeatureAugmenter:\n",
    "    \"\"\"\n",
    "    Data augmenter for text or caption features.\n",
    "\n",
    "    Applies dropout and adaptive Gaussian noise to embeddings.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    augment_prob : float\n",
    "        Probability of applying augmentation.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer applied to features.\n",
    "    noise_scale : float\n",
    "        Scaling factor for Gaussian noise.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    adaptive_noise(features):\n",
    "        Adds Gaussian noise proportional to feature's std.\n",
    "    __call__(features):\n",
    "        Applies augmentation with probability `augment_prob`.\n",
    "    \"\"\"\n",
    "    def __init__(self, augment_prob=0.3, dropout_prob=0.05, noise_scale=0.02):\n",
    "        self.augment_prob = augment_prob\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "    def adaptive_noise(self, features):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise scaled by feature standard deviation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Input feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Noisy feature tensor.\n",
    "        \"\"\"\n",
    "        std_per_feature = features.std(dim=0, keepdim=True) + 1e-6\n",
    "        noise = torch.randn_like(features) * std_per_feature * self.noise_scale\n",
    "        return features + noise\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Apply dropout and noise augmentation with probability `augment_prob`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Input feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Augmented feature tensor.\n",
    "        \"\"\"\n",
    "        if torch.rand(1) < self.augment_prob:\n",
    "            features = self.dropout(features)\n",
    "            features = self.adaptive_noise(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "# ================= DATASET =================\n",
    "class VideoTextCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for multimodal video, text, and caption features.\n",
    "\n",
    "    Handles optional data augmentation and per-sample feature normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_list : list of dict\n",
    "        List of dictionaries containing paths for 'video_feat', 'text_feat', 'caption_feat'.\n",
    "    labels : np.ndarray\n",
    "        Array of integer labels corresponding to each sample.\n",
    "    augment : bool, optional\n",
    "        Whether to apply data augmentation (default: False).\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(idx):\n",
    "        Loads, optionally augments, and returns a single sample.\n",
    "    __len__():\n",
    "        Returns the total number of samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, labels, augment=False):\n",
    "        self.data_list = data_list\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        self.video_aug = VideoFeatureAugmenter() if augment else None\n",
    "        self.text_aug = TextFeatureAugmenter() if augment else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and process a single sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary with keys: 'video_feat', 'text_feat', 'caption_feat', 'label'.\n",
    "        \"\"\"\n",
    "        item = self.data_list[idx]\n",
    "\n",
    "        video_feat = torch.from_numpy(np.load(item[\"video_feat\"])).float()\n",
    "        text_feat = torch.from_numpy(np.load(item[\"text_feat\"])).float()\n",
    "        caption_feat = torch.from_numpy(np.load(item[\"caption_feat\"])).float()\n",
    "\n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment:\n",
    "            if self.video_aug and video_feat.ndim >= 2:  \n",
    "                video_feat = self.video_aug(video_feat)\n",
    "            if self.text_aug:\n",
    "                text_feat = self.text_aug(text_feat)\n",
    "                caption_feat = self.text_aug(caption_feat)\n",
    "\n",
    "        # Flatten features if needed\n",
    "        if video_feat.ndim == 3:\n",
    "            video_feat = video_feat.mean(dim=(0,1))\n",
    "        elif video_feat.ndim == 2:\n",
    "            video_feat = video_feat.mean(dim=0)\n",
    "        \n",
    "        text_feat = text_feat if text_feat.ndim == 1 else text_feat.mean(dim=0)\n",
    "        caption_feat = caption_feat if caption_feat.ndim == 1 else caption_feat.mean(dim=0)\n",
    "\n",
    "        # Normalize each feature\n",
    "        video_feat = (video_feat - video_feat.mean()) / (video_feat.std() + 1e-6)\n",
    "        text_feat = (text_feat - text_feat.mean()) / (text_feat.std() + 1e-6)\n",
    "        caption_feat = (caption_feat - caption_feat.mean()) / (caption_feat.std() + 1e-6)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"video_feat\": video_feat,\n",
    "            \"text_feat\": text_feat,\n",
    "            \"caption_feat\": caption_feat,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "def create_dataloader(data_list, labels, batch_size=16, shuffle=True, augment=False):\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader for the multimodal dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_list : list of dict\n",
    "        List of dictionaries containing feature file paths.\n",
    "    labels : np.ndarray\n",
    "        Array of labels.\n",
    "    batch_size : int, optional\n",
    "        Batch size (default: 16).\n",
    "    shuffle : bool, optional\n",
    "        Whether to shuffle data (default: True).\n",
    "    augment : bool, optional\n",
    "        Whether to apply data augmentation (default: False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataLoader\n",
    "        PyTorch DataLoader for the dataset.\n",
    "    \"\"\"\n",
    "    dataset = VideoTextCaptionDataset(data_list, labels, augment=augment)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# ================= LOAD DATA =================\n",
    "data_list, labels = load_or_create_data(\n",
    "    json_path=\"/Users/jumita/Downloads/final_code/data.json\",\n",
    "    excel_path=\"/Users/jumita/Downloads/Book5.xlsx\",\n",
    "    labels_path=\"/Users/jumita/Downloads/final_code/labels.npy\",\n",
    "    video_dir=\"/Users/jumita/Downloads/final/frame\",\n",
    "    text_dir=\"/Users/jumita/Downloads/final/text\",\n",
    "    caption_dir=\"/Users/jumita/Downloads/final/caption\"\n",
    ")\n",
    "\n",
    "# ================= SPLIT DATA =================\n",
    "data_train, data_temp, labels_train, labels_temp = train_test_split(\n",
    "    data_list, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "try:\n",
    "    data_val, data_test, labels_val, labels_test = train_test_split(\n",
    "        data_temp, labels_temp, test_size=0.5, random_state=42, stratify=labels_temp\n",
    "    )\n",
    "except ValueError:\n",
    "    data_val, data_test, labels_val, labels_test = train_test_split(\n",
    "        data_temp, labels_temp, test_size=0.5, random_state=42, stratify=None\n",
    "    )\n",
    "\n",
    "def check_overlap(set1, set2):\n",
    "    s1 = set([item[\"video_feat\"] for item in set1])\n",
    "    s2 = set([item[\"video_feat\"] for item in set2])\n",
    "    return len(s1 & s2)\n",
    "\n",
    "print(\"Overlap train-val:\", check_overlap(data_train, data_val))\n",
    "print(\"Overlap train-test:\", check_overlap(data_train, data_test))\n",
    "print(\"Overlap val-test:\", check_overlap(data_val, data_test))\n",
    "\n",
    "\n",
    "# ================= DATALOADERS =================\n",
    "train_loader = create_dataloader(data_train, labels_train, batch_size=16, shuffle=True, augment=True)\n",
    "val_loader = create_dataloader(data_val, labels_val, batch_size=16, shuffle=False)\n",
    "test_loader = create_dataloader(data_test, labels_test, batch_size=16, shuffle=False)\n",
    "\n",
    "# ================= DEBUG SAMPLE =================\n",
    "sample = next(iter(train_loader))\n",
    "print(\"Augmented sample shapes:\")\n",
    "print(\"Video:\", sample[\"video_feat\"].shape)\n",
    "print(\"Text:\", sample[\"text_feat\"].shape)\n",
    "print(\"Caption:\", sample[\"caption_feat\"].shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a88ba6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ================= StochasticDepth =================\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Stochastic Depth (also called DropPath) regularization.\n",
    "\n",
    "    During training, randomly drops entire residual paths with probability `drop_prob`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    drop_prob : float\n",
    "        Probability of dropping a path.\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with stochastic depth.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, ...).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor, either dropped or scaled.\n",
    "        \"\"\"\n",
    "        if not self.training or self.drop_prob == 0.0:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        mask = (torch.rand(x.shape[0], device=x.device) < keep_prob).to(x.dtype)\n",
    "        mask = mask.view(-1, *([1] * (x.dim() - 1)))\n",
    "        return x * mask / keep_prob\n",
    "\n",
    "# ================= LoRA Linear =================\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with Low-Rank Adaptation (LoRA).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Input feature dimension.\n",
    "    out_features : int\n",
    "        Output feature dimension.\n",
    "    r : int\n",
    "        Rank of the LoRA adaptation matrices.\n",
    "    alpha : int\n",
    "        Scaling factor for LoRA updates.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, r=6, alpha=6):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / max(1, r)\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(torch.randn(r, in_features) * 1e-3)\n",
    "            self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through LoRA linear layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, in_features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (B, out_features).\n",
    "        \"\"\"\n",
    "        result = self.linear(x)\n",
    "        if self.r > 0:\n",
    "            lora_update = (x @ self.lora_A.T) @ self.lora_B.T\n",
    "            result += lora_update * self.scaling\n",
    "        return result\n",
    "\n",
    "# ================= LoRA MultiheadAttention =================\n",
    "class LoRAMultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention layer with LoRA adaptation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embed_dim : int\n",
    "        Dimension of embeddings.\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    dropout : float\n",
    "        Dropout probability in attention.\n",
    "    batch_first : bool\n",
    "        If True, input shape is (B, S, D).\n",
    "    r : int\n",
    "        LoRA rank.\n",
    "    alpha : int\n",
    "        LoRA scaling factor.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, batch_first=True, r=6, alpha=6):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / max(1, r)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=batch_first)\n",
    "        if r > 0:\n",
    "            self.lora_A_q = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_q = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "            self.lora_A_k = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_k = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "            self.lora_A_v = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_v = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "            self.lora_A_out = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_out = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=False, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through LoRA multihead attention.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query, key, value : torch.Tensor\n",
    "            Input tensors of shape (B, S, D).\n",
    "        key_padding_mask : torch.Tensor, optional\n",
    "            Mask for padded tokens.\n",
    "        need_weights : bool\n",
    "            If True, returns attention weights.\n",
    "        attn_mask : torch.Tensor, optional\n",
    "            Attention mask.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        attn_output : torch.Tensor\n",
    "            Attention output tensor.\n",
    "        attn_weights : torch.Tensor\n",
    "            Attention weights (if `need_weights` is True).\n",
    "        \"\"\"\n",
    "        if self.r > 0:\n",
    "            query = query + (query @ self.lora_A_q.T) @ self.lora_B_q.T * self.scaling\n",
    "            key   = key   + (key   @ self.lora_A_k.T) @ self.lora_B_k.T * self.scaling\n",
    "            value = value + (value @ self.lora_A_v.T) @ self.lora_B_v.T * self.scaling\n",
    "\n",
    "        attn_output, attn_weights = self.attn(\n",
    "            query, key, value,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=need_weights,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "\n",
    "        if self.r > 0:\n",
    "            attn_output = attn_output + (attn_output @ self.lora_A_out.T) @ self.lora_B_out.T * self.scaling\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "# ================= CoAttentionBlock =================\n",
    "class CoAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Co-Attention block between query and key/value features with LoRA and residual feed-forward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_q : int\n",
    "        Dimension of query features.\n",
    "    dim_kv : int\n",
    "        Dimension of key/value features.\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    hidden_dim : int\n",
    "        Hidden dimension for feed-forward network.\n",
    "    dropout : float\n",
    "        Dropout probability.\n",
    "    lora_r : int\n",
    "        LoRA rank.\n",
    "    drop_path_prob : float\n",
    "        Stochastic depth probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_q=256, dim_kv=256, num_heads=8, hidden_dim=256, dropout=0.2, lora_r=6, drop_path_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.query_proj = LoRALinear(dim_q, dim_q, r=lora_r)\n",
    "        self.key_proj   = LoRALinear(dim_kv, dim_q, r=lora_r)\n",
    "        self.value_proj = LoRALinear(dim_kv, dim_q, r=lora_r)\n",
    "        self.attn = LoRAMultiheadAttention(dim_q, num_heads, dropout=dropout, batch_first=True, r=lora_r)\n",
    "        self.gate = nn.Sequential(LoRALinear(dim_q * 2, dim_q, r=lora_r), nn.Sigmoid())\n",
    "        self.ffn = nn.Sequential(\n",
    "            LoRALinear(dim_q, hidden_dim, r=lora_r),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            LoRALinear(hidden_dim, dim_q, r=lora_r),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim_q)\n",
    "        self.norm2 = nn.LayerNorm(dim_q)\n",
    "        self.drop_path = StochasticDepth(drop_path_prob) if drop_path_prob > 0.0 else None\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of CoAttentionBlock.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query, key, value : torch.Tensor\n",
    "            Input feature tensors.\n",
    "        key_padding_mask : torch.Tensor, optional\n",
    "            Mask for padded tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor after co-attention, gating, and feed-forward.\n",
    "        \"\"\"\n",
    "        q = self.query_proj(query)\n",
    "        k = self.key_proj(key)\n",
    "        v = self.value_proj(value)\n",
    "        attn_out, _ = self.attn(q, k, v, key_padding_mask=key_padding_mask)\n",
    "        gate = self.gate(torch.cat([query, attn_out], dim=-1))\n",
    "        out = self.norm1(query + gate * attn_out)\n",
    "        ffn_out = self.ffn(out)\n",
    "        if self.drop_path is not None:\n",
    "            ffn_out = self.drop_path(ffn_out)\n",
    "        return self.norm2(out + ffn_out)\n",
    "\n",
    "# ================= CoAttentionLayer =================\n",
    "class CoAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Co-Attention layer for interaction between video and text features.\n",
    "\n",
    "    It applies co-attention blocks in both directions:\n",
    "    video-to-text and text-to-video, then pools the sequences\n",
    "    and concatenates the outputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_q : int\n",
    "        Dimension of query features (video features).\n",
    "    dim_kv : int\n",
    "        Dimension of key/value features (text features).\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    hidden_dim : int\n",
    "        Hidden dimension for feed-forward networks.\n",
    "    dropout : float\n",
    "        Dropout probability.\n",
    "    lora_r : int\n",
    "        LoRA rank.\n",
    "    max_video_tokens : int\n",
    "        Maximum number of video tokens to keep after pooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_q=256, dim_kv=256, num_heads=8, hidden_dim=256, dropout=0.5, lora_r=6, max_video_tokens=32):\n",
    "        super().__init__()\n",
    "        self.video2text = CoAttentionBlock(dim_q, dim_kv, num_heads, hidden_dim, dropout, lora_r)\n",
    "        self.text2video = CoAttentionBlock(dim_kv, dim_q, num_heads, hidden_dim, dropout, lora_r)\n",
    "        self.max_video_tokens = max_video_tokens\n",
    "\n",
    "    def forward(self, video_feat, text_feat, video_mask=None, text_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of co-attention layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        video_feat : torch.Tensor\n",
    "            Video features, shape (B, T, D) or (B, N, T, D) if patches.\n",
    "        text_feat : torch.Tensor\n",
    "            Text features, shape (B, S, D).\n",
    "        video_mask : torch.Tensor, optional\n",
    "            Mask for video tokens.\n",
    "        text_mask : torch.Tensor, optional\n",
    "            Mask for text tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Concatenated pooled co-attention features, shape (B, D_video + D_text).\n",
    "        \"\"\"\n",
    "        B = video_feat.shape[0]\n",
    "\n",
    "        # Flatten video patches if needed\n",
    "        if video_feat.dim() == 4:  \n",
    "            T, N, D = video_feat.shape[1], video_feat.shape[2], video_feat.shape[3]\n",
    "            video_feat = video_feat.view(B, T * N, D)\n",
    "        elif video_feat.dim() == 2:\n",
    "            video_feat = video_feat.unsqueeze(1)\n",
    "\n",
    "        # Pooling video sequence\n",
    "        if video_feat.shape[1] > self.max_video_tokens:\n",
    "            video_feat = video_feat.transpose(1, 2)  # (B, D, Seq)\n",
    "            video_feat = F.adaptive_avg_pool1d(video_feat, self.max_video_tokens)\n",
    "            video_feat = video_feat.transpose(1, 2)\n",
    "\n",
    "        if text_feat.dim() == 2:\n",
    "            text_feat = text_feat.unsqueeze(1)\n",
    "\n",
    "        # Forward co-attention with optional masks\n",
    "        v2t = self.video2text(video_feat, text_feat, text_feat, key_padding_mask=text_mask)\n",
    "        t2v = self.text2video(text_feat, video_feat, video_feat, key_padding_mask=video_mask)\n",
    "        \n",
    "        # Pool sequences\n",
    "        v2t_pooled = v2t.mean(dim=1)\n",
    "        t2v_pooled = t2v.mean(dim=1)\n",
    "        return torch.cat([v2t_pooled, t2v_pooled], dim=-1)\n",
    "\n",
    "# ================= LateFusionLoRA =================\n",
    "class LateFusionLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Late fusion module with LoRA for combining two feature modalities.\n",
    "\n",
    "    Applies two-layer MLP with LayerNorm, GELU activation, dropout,\n",
    "    and optional stochastic depth, then outputs class logits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_feat1 : int\n",
    "        Dimension of first input feature.\n",
    "    dim_feat2 : int\n",
    "        Dimension of second input feature.\n",
    "    dim_hidden : int\n",
    "        Hidden layer dimension.\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "    r : int\n",
    "        LoRA rank.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_feat1, dim_feat2, dim_hidden, num_classes, r=6):\n",
    "        super().__init__()\n",
    "        self.fc1 = LoRALinear(dim_feat1 + dim_feat2, dim_hidden * 2, r=r)\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden * 2)\n",
    "        self.fc2 = LoRALinear(dim_hidden * 2, dim_hidden, r=r)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        self.fc3 = LoRALinear(dim_hidden, num_classes, r=r)\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        self.drop_path = StochasticDepth(0.2) if dim_hidden > 256 else None\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Forward pass of late fusion module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x1, x2 : torch.Tensor\n",
    "            Input feature tensors to fuse, shape (B, dim_feat1), (B, dim_feat2).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output class logits, shape (B, num_classes).\n",
    "        \"\"\"\n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# ================= MultimodalModel =================\n",
    "class MultimodalModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal model combining co-attention between video and text features\n",
    "    and late fusion with caption features, with LoRA finetuning support.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    video_dim : int\n",
    "        Dimension of video features.\n",
    "    text_dim : int\n",
    "        Dimension of text features.\n",
    "    caption_dim : int\n",
    "        Dimension of caption features.\n",
    "    hidden_dim : int\n",
    "        Hidden dimension for co-attention and fusion layers.\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "    r : int\n",
    "        LoRA rank.\n",
    "    finetune_mode : str\n",
    "        Finetuning mode: 'coattn_plus_latefusion', 'late_fusion_only', 'coattn_only', etc.\n",
    "    dropout_prob : float\n",
    "        Dropout probability for the fusion layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, video_dim=256, text_dim=256, caption_dim=256, hidden_dim=256,\n",
    "                 num_classes=3, r=6, finetune_mode: str = 'coattn_plus_latefusion',\n",
    "                 dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.ca1 = CoAttentionLayer(video_dim, text_dim, num_heads=8, hidden_dim=hidden_dim, lora_r=r)\n",
    "        self.fc_ca1 = LoRALinear(video_dim * 2, video_dim, r=r)\n",
    "        self.dropout = nn.Dropout(dropout_prob)   \n",
    "        self.late_fusion = LateFusionLoRA(video_dim, caption_dim, hidden_dim, num_classes, r=r)\n",
    "        self.set_trainable(finetune_mode)\n",
    "\n",
    "    def set_trainable(self, finetune_mode: str = 'coattn_plus_latefusion'):\n",
    "        \"\"\"\n",
    "        Set which parameters are trainable according to finetune_mode.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        finetune_mode : str\n",
    "            Mode of finetuning.\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        finetune_mode = finetune_mode.lower()\n",
    "\n",
    "        def enable_lora(module):\n",
    "            for name, p in module.named_parameters():\n",
    "                if \"lora_A\" in name or \"lora_B\" in name:\n",
    "                    p.requires_grad = True\n",
    "\n",
    "        def unfreeze_part(module, names_to_unfreeze):\n",
    "            for name, p in module.named_parameters():\n",
    "                for target_name in names_to_unfreeze:\n",
    "                    if target_name in name:\n",
    "                        p.requires_grad = True\n",
    "\n",
    "        if finetune_mode == 'late_fusion_only':\n",
    "            enable_lora(self.late_fusion)\n",
    "            enable_lora(self.fc_ca1)\n",
    "        elif finetune_mode == 'coattn_only':\n",
    "            enable_lora(self.ca1)\n",
    "            enable_lora(self.fc_ca1)\n",
    "        elif finetune_mode == 'coattn_plus_latefusion':\n",
    "            enable_lora(self.ca1)\n",
    "            enable_lora(self.late_fusion)\n",
    "            enable_lora(self.fc_ca1)\n",
    "        elif finetune_mode == 'coattn_plus_latefusion_part':\n",
    "            enable_lora(self.ca1)\n",
    "            enable_lora(self.late_fusion)\n",
    "            enable_lora(self.fc_ca1)\n",
    "            unfreeze_part(self.ca1.video2text.query_proj, ['linear'])\n",
    "            unfreeze_part(self.late_fusion.fc1, ['linear'])\n",
    "        elif finetune_mode == 'all':\n",
    "            for p in self.parameters():\n",
    "                p.requires_grad = True\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown finetune_mode: {finetune_mode}\")\n",
    "\n",
    "    def get_param_groups(self, lr_late=2e-4, lr_coattn=1.5e-4, lr_fc_ca1=5e-5, weight_decay=5e-4):\n",
    "        \"\"\"\n",
    "        Return parameter groups with separate learning rates for optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr_late, lr_coattn, lr_fc_ca1 : float\n",
    "            Learning rates for respective modules.\n",
    "        weight_decay : float\n",
    "            Weight decay for optimizer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of dict\n",
    "            Parameter groups for optimizer.\n",
    "        \"\"\"\n",
    "        groups = []\n",
    "        if any(p.requires_grad for p in self.late_fusion.parameters()):\n",
    "            groups.append({'params': [p for p in self.late_fusion.parameters() if p.requires_grad], 'lr': lr_late, 'weight_decay': weight_decay})\n",
    "        if any(p.requires_grad for p in self.ca1.parameters()):\n",
    "            groups.append({'params': [p for p in self.ca1.parameters() if p.requires_grad], 'lr': lr_coattn, 'weight_decay': weight_decay})\n",
    "        if any(p.requires_grad for p in self.fc_ca1.parameters()):\n",
    "            groups.append({'params': [p for p in self.fc_ca1.parameters() if p.requires_grad], 'lr': lr_fc_ca1, 'weight_decay': weight_decay})\n",
    "        return groups\n",
    "\n",
    "    def forward(self, video_feat, text_feat, caption_feat, video_mask=None, text_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the multimodal model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        video_feat : torch.Tensor\n",
    "            Video features.\n",
    "        text_feat : torch.Tensor\n",
    "            Text features.\n",
    "        caption_feat : torch.Tensor\n",
    "            Caption features.\n",
    "        video_mask : torch.Tensor, optional\n",
    "            Mask for video tokens.\n",
    "        text_mask : torch.Tensor, optional\n",
    "            Mask for text tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Class logits for each sample.\n",
    "        \"\"\"\n",
    "        ca1_out = self.ca1(video_feat, text_feat, video_mask=video_mask, text_mask=text_mask)\n",
    "        ca1_out = self.fc_ca1(ca1_out)\n",
    "        ca1_out = self.dropout(ca1_out)\n",
    "        return self.late_fusion(ca1_out, caption_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "218678d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Ablation: without_coattn ===\n",
      "Epoch 1/25 | Train Loss: 0.9996, Acc: 0.5177 | Val Acc: 0.5861, F1: 0.5858\n",
      "Epoch 2/25 | Train Loss: 0.9031, Acc: 0.6019 | Val Acc: 0.6554, F1: 0.6553\n",
      "Epoch 3/25 | Train Loss: 0.8548, Acc: 0.6393 | Val Acc: 0.6396, F1: 0.6383\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 4/25 | Train Loss: 0.8258, Acc: 0.6542 | Val Acc: 0.6535, F1: 0.6545\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 5/25 | Train Loss: 0.7914, Acc: 0.6901 | Val Acc: 0.6693, F1: 0.6684\n",
      "Epoch 6/25 | Train Loss: 0.7784, Acc: 0.6953 | Val Acc: 0.6653, F1: 0.6658\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 7/25 | Train Loss: 0.7517, Acc: 0.7154 | Val Acc: 0.6772, F1: 0.6769\n",
      "Epoch 8/25 | Train Loss: 0.7326, Acc: 0.7275 | Val Acc: 0.6871, F1: 0.6865\n",
      "Epoch 9/25 | Train Loss: 0.7186, Acc: 0.7317 | Val Acc: 0.6871, F1: 0.6866\n",
      "Epoch 10/25 | Train Loss: 0.7015, Acc: 0.7451 | Val Acc: 0.6911, F1: 0.6904\n",
      "Epoch 11/25 | Train Loss: 0.6940, Acc: 0.7511 | Val Acc: 0.6970, F1: 0.6953\n",
      "Epoch 12/25 | Train Loss: 0.6712, Acc: 0.7721 | Val Acc: 0.6970, F1: 0.6970\n",
      "Epoch 13/25 | Train Loss: 0.6573, Acc: 0.7785 | Val Acc: 0.7030, F1: 0.7024\n",
      "Epoch 14/25 | Train Loss: 0.6525, Acc: 0.7805 | Val Acc: 0.7010, F1: 0.6997\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 15/25 | Train Loss: 0.6375, Acc: 0.7877 | Val Acc: 0.6911, F1: 0.6902\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 16/25 | Train Loss: 0.6371, Acc: 0.7778 | Val Acc: 0.6931, F1: 0.6930\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 17/25 | Train Loss: 0.6157, Acc: 0.8006 | Val Acc: 0.6851, F1: 0.6856\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 18/25 | Train Loss: 0.6122, Acc: 0.8063 | Val Acc: 0.7188, F1: 0.7154\n",
      "Epoch 19/25 | Train Loss: 0.6002, Acc: 0.8053 | Val Acc: 0.7208, F1: 0.7211\n",
      "Epoch 20/25 | Train Loss: 0.5888, Acc: 0.8132 | Val Acc: 0.7129, F1: 0.7130\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 21/25 | Train Loss: 0.5810, Acc: 0.8271 | Val Acc: 0.6990, F1: 0.6987\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 22/25 | Train Loss: 0.5786, Acc: 0.8226 | Val Acc: 0.7208, F1: 0.7206\n",
      "Epoch 23/25 | Train Loss: 0.5635, Acc: 0.8345 | Val Acc: 0.6693, F1: 0.6688\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 24/25 | Train Loss: 0.5612, Acc: 0.8345 | Val Acc: 0.7188, F1: 0.7181\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 25/25 | Train Loss: 0.5564, Acc: 0.8402 | Val Acc: 0.7089, F1: 0.7084\n",
      "EarlyStopping counter: 3 / 10\n",
      "\n",
      "Training completed in 33m 40s. Best Val Acc: 0.7208\n",
      "\n",
      "=== Running Ablation: without_latefusion ===\n",
      "Epoch 1/25 | Train Loss: 1.1353, Acc: 0.3285 | Val Acc: 0.3822, F1: 0.3571\n",
      "Epoch 2/25 | Train Loss: 1.0932, Acc: 0.3981 | Val Acc: 0.4614, F1: 0.4554\n",
      "Epoch 3/25 | Train Loss: 1.0272, Acc: 0.4882 | Val Acc: 0.5188, F1: 0.5123\n",
      "Epoch 4/25 | Train Loss: 0.9863, Acc: 0.5289 | Val Acc: 0.5703, F1: 0.5658\n",
      "Epoch 5/25 | Train Loss: 0.9636, Acc: 0.5524 | Val Acc: 0.5901, F1: 0.5877\n",
      "Epoch 6/25 | Train Loss: 0.9394, Acc: 0.5712 | Val Acc: 0.6000, F1: 0.5981\n",
      "Epoch 7/25 | Train Loss: 0.9286, Acc: 0.5895 | Val Acc: 0.6198, F1: 0.6189\n",
      "Epoch 8/25 | Train Loss: 0.9204, Acc: 0.5898 | Val Acc: 0.6158, F1: 0.6149\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 9/25 | Train Loss: 0.9096, Acc: 0.6039 | Val Acc: 0.6119, F1: 0.6110\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 10/25 | Train Loss: 0.8976, Acc: 0.6121 | Val Acc: 0.6139, F1: 0.6128\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 11/25 | Train Loss: 0.8903, Acc: 0.6205 | Val Acc: 0.6059, F1: 0.6043\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 12/25 | Train Loss: 0.8915, Acc: 0.6131 | Val Acc: 0.6000, F1: 0.5985\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 13/25 | Train Loss: 0.8770, Acc: 0.6331 | Val Acc: 0.6059, F1: 0.6047\n",
      "EarlyStopping counter: 6 / 10\n",
      "Epoch 14/25 | Train Loss: 0.8711, Acc: 0.6458 | Val Acc: 0.6040, F1: 0.6026\n",
      "EarlyStopping counter: 7 / 10\n",
      "Epoch 15/25 | Train Loss: 0.8696, Acc: 0.6366 | Val Acc: 0.5980, F1: 0.5972\n",
      "EarlyStopping counter: 8 / 10\n",
      "Epoch 16/25 | Train Loss: 0.8638, Acc: 0.6423 | Val Acc: 0.6020, F1: 0.6015\n",
      "EarlyStopping counter: 9 / 10\n",
      "Epoch 17/25 | Train Loss: 0.8601, Acc: 0.6448 | Val Acc: 0.6000, F1: 0.5995\n",
      "EarlyStopping counter: 10 / 10\n",
      "Early stopping at epoch 17. Best Val Acc: 0.6198\n",
      "\n",
      "Training completed in 23m 47s. Best Val Acc: 0.6198\n",
      "\n",
      "=== Running Ablation: without_video ===\n",
      "Epoch 1/25 | Train Loss: 1.0350, Acc: 0.4974 | Val Acc: 0.5307, F1: 0.5255\n",
      "Epoch 2/25 | Train Loss: 0.9868, Acc: 0.5348 | Val Acc: 0.5743, F1: 0.5729\n",
      "Epoch 3/25 | Train Loss: 0.9606, Acc: 0.5591 | Val Acc: 0.5644, F1: 0.5606\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 4/25 | Train Loss: 0.9378, Acc: 0.5824 | Val Acc: 0.5921, F1: 0.5907\n",
      "Epoch 5/25 | Train Loss: 0.9294, Acc: 0.5923 | Val Acc: 0.5723, F1: 0.5700\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 6/25 | Train Loss: 0.9037, Acc: 0.6004 | Val Acc: 0.5802, F1: 0.5799\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 7/25 | Train Loss: 0.8887, Acc: 0.6151 | Val Acc: 0.5802, F1: 0.5794\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 8/25 | Train Loss: 0.8737, Acc: 0.6205 | Val Acc: 0.5782, F1: 0.5724\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 9/25 | Train Loss: 0.8650, Acc: 0.6344 | Val Acc: 0.5663, F1: 0.5638\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 10/25 | Train Loss: 0.8550, Acc: 0.6356 | Val Acc: 0.5980, F1: 0.5974\n",
      "Epoch 11/25 | Train Loss: 0.8448, Acc: 0.6406 | Val Acc: 0.6059, F1: 0.6054\n",
      "Epoch 12/25 | Train Loss: 0.8305, Acc: 0.6530 | Val Acc: 0.6020, F1: 0.6015\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 13/25 | Train Loss: 0.8226, Acc: 0.6604 | Val Acc: 0.6000, F1: 0.5985\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 14/25 | Train Loss: 0.8124, Acc: 0.6698 | Val Acc: 0.6040, F1: 0.6027\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 15/25 | Train Loss: 0.8092, Acc: 0.6698 | Val Acc: 0.6079, F1: 0.6070\n",
      "Epoch 16/25 | Train Loss: 0.7984, Acc: 0.6839 | Val Acc: 0.6218, F1: 0.6215\n",
      "Epoch 17/25 | Train Loss: 0.7957, Acc: 0.6839 | Val Acc: 0.6040, F1: 0.6026\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 18/25 | Train Loss: 0.7896, Acc: 0.6921 | Val Acc: 0.6238, F1: 0.6226\n",
      "Epoch 19/25 | Train Loss: 0.7765, Acc: 0.7023 | Val Acc: 0.5901, F1: 0.5907\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 20/25 | Train Loss: 0.7733, Acc: 0.6956 | Val Acc: 0.6119, F1: 0.6104\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 21/25 | Train Loss: 0.7588, Acc: 0.7030 | Val Acc: 0.6040, F1: 0.6027\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 22/25 | Train Loss: 0.7551, Acc: 0.7027 | Val Acc: 0.6020, F1: 0.6020\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 23/25 | Train Loss: 0.7574, Acc: 0.7040 | Val Acc: 0.6079, F1: 0.6078\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 24/25 | Train Loss: 0.7451, Acc: 0.7139 | Val Acc: 0.6000, F1: 0.6003\n",
      "EarlyStopping counter: 6 / 10\n",
      "Epoch 25/25 | Train Loss: 0.7180, Acc: 0.7397 | Val Acc: 0.6139, F1: 0.6138\n",
      "EarlyStopping counter: 7 / 10\n",
      "\n",
      "Training completed in 34m 44s. Best Val Acc: 0.6238\n",
      "\n",
      "=== Running Ablation: without_text ===\n",
      "Epoch 1/25 | Train Loss: 1.0062, Acc: 0.5224 | Val Acc: 0.6079, F1: 0.6083\n",
      "Epoch 2/25 | Train Loss: 0.8980, Acc: 0.6158 | Val Acc: 0.6455, F1: 0.6454\n",
      "Epoch 3/25 | Train Loss: 0.8541, Acc: 0.6359 | Val Acc: 0.6436, F1: 0.6442\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 4/25 | Train Loss: 0.8278, Acc: 0.6562 | Val Acc: 0.6614, F1: 0.6627\n",
      "Epoch 5/25 | Train Loss: 0.8019, Acc: 0.6745 | Val Acc: 0.6713, F1: 0.6719\n",
      "Epoch 6/25 | Train Loss: 0.7816, Acc: 0.6928 | Val Acc: 0.6594, F1: 0.6610\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 7/25 | Train Loss: 0.7664, Acc: 0.7080 | Val Acc: 0.6713, F1: 0.6715\n",
      "Epoch 8/25 | Train Loss: 0.7539, Acc: 0.7144 | Val Acc: 0.6673, F1: 0.6676\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 9/25 | Train Loss: 0.7307, Acc: 0.7231 | Val Acc: 0.6554, F1: 0.6536\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 10/25 | Train Loss: 0.7160, Acc: 0.7431 | Val Acc: 0.6733, F1: 0.6725\n",
      "Epoch 11/25 | Train Loss: 0.6985, Acc: 0.7454 | Val Acc: 0.6851, F1: 0.6845\n",
      "Epoch 12/25 | Train Loss: 0.6951, Acc: 0.7520 | Val Acc: 0.6832, F1: 0.6823\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 13/25 | Train Loss: 0.6842, Acc: 0.7545 | Val Acc: 0.6891, F1: 0.6884\n",
      "Epoch 14/25 | Train Loss: 0.6684, Acc: 0.7659 | Val Acc: 0.6832, F1: 0.6829\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 15/25 | Train Loss: 0.6575, Acc: 0.7719 | Val Acc: 0.6752, F1: 0.6732\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 16/25 | Train Loss: 0.6503, Acc: 0.7758 | Val Acc: 0.6871, F1: 0.6857\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 17/25 | Train Loss: 0.6433, Acc: 0.7820 | Val Acc: 0.6752, F1: 0.6749\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 18/25 | Train Loss: 0.6282, Acc: 0.7912 | Val Acc: 0.7010, F1: 0.7005\n",
      "Epoch 19/25 | Train Loss: 0.6188, Acc: 0.7937 | Val Acc: 0.6832, F1: 0.6824\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 20/25 | Train Loss: 0.6166, Acc: 0.8013 | Val Acc: 0.6851, F1: 0.6846\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 21/25 | Train Loss: 0.6047, Acc: 0.8115 | Val Acc: 0.6970, F1: 0.6966\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 22/25 | Train Loss: 0.5920, Acc: 0.8150 | Val Acc: 0.6812, F1: 0.6784\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 23/25 | Train Loss: 0.5840, Acc: 0.8132 | Val Acc: 0.6851, F1: 0.6812\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 24/25 | Train Loss: 0.5777, Acc: 0.8278 | Val Acc: 0.6990, F1: 0.6968\n",
      "EarlyStopping counter: 6 / 10\n",
      "Epoch 25/25 | Train Loss: 0.5589, Acc: 0.8360 | Val Acc: 0.6931, F1: 0.6916\n",
      "EarlyStopping counter: 7 / 10\n",
      "\n",
      "Training completed in 34m 59s. Best Val Acc: 0.7010\n",
      "\n",
      "=== Running Ablation: without_caption ===\n",
      "Epoch 1/25 | Train Loss: 1.0409, Acc: 0.4746 | Val Acc: 0.5426, F1: 0.5426\n",
      "Epoch 2/25 | Train Loss: 0.9414, Acc: 0.5767 | Val Acc: 0.5683, F1: 0.5686\n",
      "Epoch 3/25 | Train Loss: 0.8937, Acc: 0.6099 | Val Acc: 0.5980, F1: 0.5981\n",
      "Epoch 4/25 | Train Loss: 0.8623, Acc: 0.6252 | Val Acc: 0.6337, F1: 0.6331\n",
      "Epoch 5/25 | Train Loss: 0.8365, Acc: 0.6522 | Val Acc: 0.6356, F1: 0.6359\n",
      "Epoch 6/25 | Train Loss: 0.8170, Acc: 0.6624 | Val Acc: 0.6257, F1: 0.6257\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 7/25 | Train Loss: 0.7998, Acc: 0.6733 | Val Acc: 0.6535, F1: 0.6531\n",
      "Epoch 8/25 | Train Loss: 0.7929, Acc: 0.6829 | Val Acc: 0.6693, F1: 0.6682\n",
      "Epoch 9/25 | Train Loss: 0.7815, Acc: 0.6859 | Val Acc: 0.6713, F1: 0.6709\n",
      "Epoch 10/25 | Train Loss: 0.7621, Acc: 0.7025 | Val Acc: 0.6634, F1: 0.6626\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 11/25 | Train Loss: 0.7509, Acc: 0.7077 | Val Acc: 0.6475, F1: 0.6472\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 12/25 | Train Loss: 0.7493, Acc: 0.7132 | Val Acc: 0.6554, F1: 0.6548\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 13/25 | Train Loss: 0.7364, Acc: 0.7144 | Val Acc: 0.6634, F1: 0.6624\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 14/25 | Train Loss: 0.7204, Acc: 0.7260 | Val Acc: 0.6614, F1: 0.6617\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 15/25 | Train Loss: 0.7158, Acc: 0.7424 | Val Acc: 0.6733, F1: 0.6725\n",
      "Epoch 16/25 | Train Loss: 0.7023, Acc: 0.7553 | Val Acc: 0.6594, F1: 0.6580\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 17/25 | Train Loss: 0.6970, Acc: 0.7498 | Val Acc: 0.6495, F1: 0.6489\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 18/25 | Train Loss: 0.6838, Acc: 0.7587 | Val Acc: 0.6356, F1: 0.6347\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 19/25 | Train Loss: 0.6827, Acc: 0.7555 | Val Acc: 0.6614, F1: 0.6604\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 20/25 | Train Loss: 0.6708, Acc: 0.7664 | Val Acc: 0.6574, F1: 0.6566\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 21/25 | Train Loss: 0.6553, Acc: 0.7773 | Val Acc: 0.6495, F1: 0.6492\n",
      "EarlyStopping counter: 6 / 10\n",
      "Epoch 22/25 | Train Loss: 0.6467, Acc: 0.7845 | Val Acc: 0.6614, F1: 0.6605\n",
      "EarlyStopping counter: 7 / 10\n",
      "Epoch 23/25 | Train Loss: 0.6459, Acc: 0.7810 | Val Acc: 0.6515, F1: 0.6512\n",
      "EarlyStopping counter: 8 / 10\n",
      "Epoch 24/25 | Train Loss: 0.6395, Acc: 0.7892 | Val Acc: 0.6792, F1: 0.6790\n",
      "Epoch 25/25 | Train Loss: 0.6323, Acc: 0.7919 | Val Acc: 0.6673, F1: 0.6667\n",
      "EarlyStopping counter: 1 / 10\n",
      "\n",
      "Training completed in 34m 40s. Best Val Acc: 0.6792\n",
      "\n",
      "=== Running Ablation: without_lora ===\n",
      "Epoch 1/25 | Train Loss: 0.9790, Acc: 0.5368 | Val Acc: 0.6277, F1: 0.6251\n",
      "Epoch 2/25 | Train Loss: 0.8478, Acc: 0.6453 | Val Acc: 0.6337, F1: 0.6336\n",
      "Epoch 3/25 | Train Loss: 0.8052, Acc: 0.6703 | Val Acc: 0.6396, F1: 0.6398\n",
      "Epoch 4/25 | Train Loss: 0.7507, Acc: 0.7124 | Val Acc: 0.6851, F1: 0.6852\n",
      "Epoch 5/25 | Train Loss: 0.7216, Acc: 0.7322 | Val Acc: 0.6535, F1: 0.6533\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 6/25 | Train Loss: 0.6924, Acc: 0.7520 | Val Acc: 0.6752, F1: 0.6746\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 7/25 | Train Loss: 0.6542, Acc: 0.7773 | Val Acc: 0.6911, F1: 0.6920\n",
      "Epoch 8/25 | Train Loss: 0.6300, Acc: 0.7959 | Val Acc: 0.6871, F1: 0.6872\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 9/25 | Train Loss: 0.6050, Acc: 0.8060 | Val Acc: 0.6812, F1: 0.6829\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 10/25 | Train Loss: 0.5864, Acc: 0.8187 | Val Acc: 0.6574, F1: 0.6576\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 11/25 | Train Loss: 0.5732, Acc: 0.8229 | Val Acc: 0.6832, F1: 0.6840\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 12/25 | Train Loss: 0.5432, Acc: 0.8457 | Val Acc: 0.6792, F1: 0.6781\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 13/25 | Train Loss: 0.5225, Acc: 0.8546 | Val Acc: 0.6713, F1: 0.6709\n",
      "EarlyStopping counter: 6 / 10\n",
      "Epoch 14/25 | Train Loss: 0.4748, Acc: 0.8875 | Val Acc: 0.6792, F1: 0.6799\n",
      "EarlyStopping counter: 7 / 10\n",
      "Epoch 15/25 | Train Loss: 0.4513, Acc: 0.8977 | Val Acc: 0.6832, F1: 0.6832\n",
      "EarlyStopping counter: 8 / 10\n",
      "Epoch 16/25 | Train Loss: 0.4455, Acc: 0.9066 | Val Acc: 0.6673, F1: 0.6675\n",
      "EarlyStopping counter: 9 / 10\n",
      "Epoch 17/25 | Train Loss: 0.4360, Acc: 0.9069 | Val Acc: 0.6970, F1: 0.6975\n",
      "Epoch 18/25 | Train Loss: 0.4194, Acc: 0.9175 | Val Acc: 0.6614, F1: 0.6624\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 19/25 | Train Loss: 0.4155, Acc: 0.9227 | Val Acc: 0.6713, F1: 0.6716\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 20/25 | Train Loss: 0.4002, Acc: 0.9304 | Val Acc: 0.6713, F1: 0.6720\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 21/25 | Train Loss: 0.3951, Acc: 0.9301 | Val Acc: 0.6792, F1: 0.6792\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 22/25 | Train Loss: 0.3864, Acc: 0.9398 | Val Acc: 0.6792, F1: 0.6804\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 23/25 | Train Loss: 0.3802, Acc: 0.9410 | Val Acc: 0.6772, F1: 0.6774\n",
      "EarlyStopping counter: 6 / 10\n",
      "Epoch 24/25 | Train Loss: 0.3580, Acc: 0.9604 | Val Acc: 0.6752, F1: 0.6761\n",
      "EarlyStopping counter: 7 / 10\n",
      "Epoch 25/25 | Train Loss: 0.3567, Acc: 0.9589 | Val Acc: 0.6693, F1: 0.6695\n",
      "EarlyStopping counter: 8 / 10\n",
      "\n",
      "Training completed in 36m 3s. Best Val Acc: 0.6970\n",
      "\n",
      "=== Ablation Table (Test Set) ===\n",
      "Mode                 | Acc    | F1     | Prec   | Recall\n",
      "------------------------------------------------------------\n",
      "without_coattn       | 0.6634 | 0.6638 | 0.6650 | 0.6684\n",
      "without_latefusion   | 0.5723 | 0.5682 | 0.5772 | 0.5804\n",
      "without_video        | 0.6020 | 0.6012 | 0.6086 | 0.6020\n",
      "without_text         | 0.6554 | 0.6541 | 0.6597 | 0.6620\n",
      "without_caption      | 0.6218 | 0.6209 | 0.6214 | 0.6261\n",
      "without_lora         | 0.6495 | 0.6490 | 0.6493 | 0.6541\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import os, time\n",
    "\n",
    "# ============================= TRAINING FUNCTION =============================\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Performs one training epoch for the given model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The multimodal model to train.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for model parameters.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to run the computations on.\n",
    "\n",
    "    Returns:\n",
    "        float: Average training loss over the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        video_feat = batch[\"video_feat\"].to(device)\n",
    "        text_feat = batch[\"text_feat\"].to(device)\n",
    "        caption_feat = batch[\"caption_feat\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(video_feat, text_feat, caption_feat)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * video_feat.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# ============================= Metric Evaluation =============================\n",
    "def compute_metrics(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Computes accuracy, F1, precision, and recall for a given model on a dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model for evaluation.\n",
    "        dataloader (DataLoader): DataLoader for evaluation data.\n",
    "        device (torch.device): Device to run the computations on.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing 'acc', 'f1', 'precision', and 'recall'.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            video_feat = batch[\"video_feat\"].to(device)\n",
    "            text_feat = batch[\"text_feat\"].to(device)\n",
    "            caption_feat = batch[\"caption_feat\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(video_feat, text_feat, caption_feat)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return {\"acc\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# ============================= Ablation Model Wrapper =============================\n",
    "class MultimodalModelWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for a multimodal model that allows ablation studies\n",
    "    by disabling certain modalities or LoRA components.\n",
    "\n",
    "    Args:\n",
    "        base_model_cls (nn.Module): Base multimodal model class.\n",
    "        mode (str): Ablation mode ('full', 'without_video', 'without_text', 'without_caption', 'without_coattn', 'without_latefusion', 'without_lora').\n",
    "        r (int): LoRA rank for low-rank adaptation layers.\n",
    "        use_video (bool): Whether to use video modality.\n",
    "        use_text (bool): Whether to use text modality.\n",
    "        use_caption (bool): Whether to use caption modality.\n",
    "        kwargs: Additional keyword arguments passed to the base model.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model_cls, mode=\"full\", r=6,\n",
    "                 use_video=True, use_text=True, use_caption=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mode = mode.lower()\n",
    "        self.use_video = use_video\n",
    "        self.use_text = use_text\n",
    "        self.use_caption = use_caption\n",
    "\n",
    "        self.model = base_model_cls(r=r, **kwargs)\n",
    "        self.set_trainable_fixed(self.mode)\n",
    "\n",
    "    def set_trainable_fixed(self, mode):\n",
    "        \"\"\"\n",
    "        Freezes or unfreezes layers according to the ablation mode.\n",
    "        \"\"\"\n",
    "        # First freeze all parameters\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        def enable_lora(module):\n",
    "            if module is not None:\n",
    "                for name, p in module.named_parameters():\n",
    "                    if \"lora_\" in name.lower():\n",
    "                        p.requires_grad = True\n",
    "\n",
    "        def unfreeze_linear(module, names_to_unfreeze):\n",
    "            if module is not None:\n",
    "                for name, p in module.named_parameters():\n",
    "                    if any(n in name for n in names_to_unfreeze):\n",
    "                        p.requires_grad = True\n",
    "\n",
    "        if mode == \"without_coattn\":\n",
    "            if hasattr(self.model, 'ca1'):\n",
    "                for p in self.model.ca1.parameters():\n",
    "                    p.requires_grad = False\n",
    "            enable_lora(getattr(self.model, 'late_fusion', None))\n",
    "            enable_lora(getattr(self.model, 'fc_ca1', None))\n",
    "            unfreeze_linear(getattr(self.model.late_fusion, 'fc1', None), ['linear'])\n",
    "\n",
    "        elif mode == \"without_latefusion\":\n",
    "            if hasattr(self.model, 'late_fusion'):\n",
    "                for p in self.model.late_fusion.parameters():\n",
    "                    p.requires_grad = False\n",
    "            enable_lora(getattr(self.model, 'ca1', None))\n",
    "            enable_lora(getattr(self.model, 'fc_ca1', None))\n",
    "            unfreeze_linear(getattr(self.model.ca1.video2text, 'query_proj', None), ['linear'])\n",
    "\n",
    "        elif mode in [\"without_video\", \"without_text\", \"without_caption\"]:\n",
    "            enable_lora(getattr(self.model, 'ca1', None))\n",
    "            enable_lora(getattr(self.model, 'late_fusion', None))\n",
    "            enable_lora(getattr(self.model, 'fc_ca1', None))\n",
    "            unfreeze_linear(getattr(self.model.ca1.video2text, 'query_proj', None), ['linear'])\n",
    "            unfreeze_linear(getattr(self.model.late_fusion, 'fc1', None), ['linear'])\n",
    "\n",
    "        elif mode == \"without_lora\":\n",
    "            # Do not use LoRA\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        else:  # full mode\n",
    "            enable_lora(getattr(self.model, 'ca1', None))\n",
    "            enable_lora(getattr(self.model, 'late_fusion', None))\n",
    "            enable_lora(getattr(self.model, 'fc_ca1', None))\n",
    "            unfreeze_linear(getattr(self.model.ca1.video2text, 'query_proj', None), ['linear'])\n",
    "            unfreeze_linear(getattr(self.model.late_fusion, 'fc1', None), ['linear'])\n",
    "\n",
    "    def forward(self, video_feat, text_feat, caption_feat, video_mask=None, text_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model with optional modality masking.\n",
    "\n",
    "        Args:\n",
    "            video_feat (Tensor): Video features tensor.\n",
    "            text_feat (Tensor): Text features tensor.\n",
    "            caption_feat (Tensor): Caption features tensor.\n",
    "            video_mask (Tensor, optional): Mask for video tokens.\n",
    "            text_mask (Tensor, optional): Mask for text tokens.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits from the model.\n",
    "        \"\"\"\n",
    "        if not self.use_video:\n",
    "            video_feat = torch.zeros_like(video_feat).to(video_feat.device)\n",
    "            video_mask = None\n",
    "        if not self.use_text:\n",
    "            text_feat = torch.zeros_like(text_feat).to(text_feat.device)\n",
    "            text_mask = None\n",
    "        if not self.use_caption:\n",
    "            caption_feat = torch.zeros_like(caption_feat).to(caption_feat.device)\n",
    "\n",
    "        return self.model(video_feat, text_feat, caption_feat,\n",
    "                          video_mask=video_mask, text_mask=text_mask)\n",
    "\n",
    "# ============================= Early Stopping =============================\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping utility to stop training when validation metric stops improving.\n",
    "\n",
    "    Args:\n",
    "        patience (int): Number of epochs with no improvement after which training is stopped.\n",
    "        delta (float): Minimum change in the monitored metric to qualify as improvement.\n",
    "        verbose (bool): Whether to print early stopping messages.\n",
    "        mode (str): 'min' for loss, 'max' for accuracy or metric to maximize.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, delta=0.0, verbose=False, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, metric_value):\n",
    "        \"\"\"\n",
    "        Checks if training should stop based on the provided metric.\n",
    "\n",
    "        Args:\n",
    "            metric_value (float): Current value of the monitored metric.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if training should stop, False otherwise.\n",
    "        \"\"\"\n",
    "        if self.mode == 'min':\n",
    "            score = -metric_value\n",
    "        else:\n",
    "            score = metric_value\n",
    "            \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "# ============================= Trainer =============================\n",
    "class MultimodalTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for multimodal models with gradient accumulation, early stopping, and checkpointing.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to train.\n",
    "        device (torch.device): Device for training.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler.\n",
    "        grad_accum_steps (int): Number of steps to accumulate gradients.\n",
    "        max_grad_norm (float): Maximum gradient norm for clipping.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device, optimizer, criterion, scheduler=None, grad_accum_steps=2, max_grad_norm=1.0):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.grad_accum_steps = grad_accum_steps\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.best_metrics = {'acc': 0.0, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0, 'epoch': 0}\n",
    "\n",
    "    def _to_device(self, batch):\n",
    "        \"\"\"\n",
    "        Moves batch data to the specified device.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): Dictionary of tensors.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary with tensors on the correct device.\n",
    "        \"\"\"\n",
    "        return {k: v.to(self.device) for k, v in batch.items()}\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Performs a single training epoch with optional gradient accumulation.\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader): DataLoader for training data.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - float: Average training loss over the epoch.\n",
    "                - float: Training accuracy over the epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch = self._to_device(batch)\n",
    "            outputs = self.model(batch['video_feat'], batch['text_feat'], batch['caption_feat'])\n",
    "            loss = self.criterion(outputs, batch['label']) / self.grad_accum_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % self.grad_accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * batch['label'].size(0) * self.grad_accum_steps\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch['label']).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "            \n",
    "        # Handle remaining gradients if any\n",
    "        if total % self.grad_accum_steps != 0:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        return total_loss / total, correct / total\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the validation set.\n",
    "\n",
    "        Args:\n",
    "            val_loader (DataLoader): DataLoader for validation data.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing validation metrics: 'acc', 'f1', 'precision', 'recall'.\n",
    "        \"\"\"\n",
    "        return compute_metrics(self.model, val_loader, self.device)\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs=25, patience=10, checkpoint_dir=\"checkpoints\"):\n",
    "        \"\"\"\n",
    "        Trains the model over multiple epochs, applying early stopping and checkpointing.\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader): DataLoader for training data.\n",
    "            val_loader (DataLoader): DataLoader for validation data.\n",
    "            num_epochs (int): Maximum number of epochs to train.\n",
    "            patience (int): Number of epochs to wait for improvement before early stopping.\n",
    "            checkpoint_dir (str): Directory to save the best model checkpoint.\n",
    "\n",
    "        Returns:\n",
    "            dict: Best validation metrics achieved during training including 'acc', 'f1', 'precision', 'recall', 'epoch'.\n",
    "        \"\"\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True, mode='max')\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            val_metrics = self.validate(val_loader)\n",
    "            val_acc = val_metrics[\"acc\"]\n",
    "\n",
    "            metric_for_scheduler = 1 - val_acc\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step(metric_for_scheduler)\n",
    "            # Update best metrics\n",
    "            if val_acc > self.best_metrics['acc']:\n",
    "                self.best_metrics = val_metrics.copy()\n",
    "                self.best_metrics['epoch'] = epoch + 1\n",
    "                torch.save(self.model.state_dict(), checkpoint_path)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Acc: {val_metrics['acc']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "\n",
    "            if early_stopping(val_acc):\n",
    "                print(f\"Early stopping at epoch {epoch+1}. Best Val Acc: {self.best_metrics['acc']:.4f}\")\n",
    "                break\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {total_time//60:.0f}m {total_time%60:.0f}s. Best Val Acc: {self.best_metrics['acc']:.4f}\")\n",
    "\n",
    "        # Load best model\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            self.model.load_state_dict(torch.load(checkpoint_path, map_location=self.device))\n",
    "        return self.best_metrics\n",
    "\n",
    "# ============================= Ablation Runner =============================\n",
    "def run_ablation(base_model_cls, train_loader, val_loader, test_loader,\n",
    "                 device, num_classes=3, num_epochs=25, patience=10, hidden_dim=256):\n",
    "    \"\"\"\n",
    "    Performs an ablation study by training and evaluating the model with different components removed.\n",
    "\n",
    "    Args:\n",
    "        base_model_cls (nn.Module): Base multimodal model class.\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        val_loader (DataLoader): Validation data loader.\n",
    "        test_loader (DataLoader): Test data loader.\n",
    "        device (torch.device): Device for training and evaluation.\n",
    "        num_classes (int): Number of output classes.\n",
    "        num_epochs (int): Maximum number of epochs to train.\n",
    "        patience (int): Patience for early stopping.\n",
    "        hidden_dim (int): Hidden dimension size for intermediate layers.\n",
    "\n",
    "    Returns:\n",
    "        dict: Returns results for each ablation mode, including validation and test metrics.\n",
    "    \"\"\"\n",
    "    ablation_modes = [\n",
    "        (\"without_coattn\", True, True, True),\n",
    "        (\"without_latefusion\", True, True, True),\n",
    "        (\"without_video\", False, True, True),\n",
    "        (\"without_text\", True, False, True),\n",
    "        (\"without_caption\", True, True, False),\n",
    "        (\"without_lora\", True, True, True), \n",
    "    ]\n",
    "\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for mode_name, use_video, use_text, use_caption in ablation_modes:\n",
    "        print(f\"\\n=== Running Ablation: {mode_name} ===\")\n",
    "\n",
    "        model = MultimodalModelWrapper(\n",
    "            base_model_cls,\n",
    "            mode=mode_name,\n",
    "            r=6,\n",
    "            video_dim=256,\n",
    "            text_dim=256,\n",
    "            caption_dim=256,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_classes=num_classes,\n",
    "            use_video=use_video,\n",
    "            use_text=use_text,\n",
    "            use_caption=use_caption\n",
    "        )\n",
    "\n",
    "        class_weights = torch.tensor([1.22, 0.90, 0.94], device=device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.08)\n",
    "        params_to_update = [\n",
    "            {\"params\": model.model.ca1.parameters(), \"lr\": 1.5e-4},\n",
    "            {\"params\": model.model.late_fusion.parameters(), \"lr\": 1.5e-4},\n",
    "            {\"params\": model.model.fc_ca1.parameters(), \"lr\": 5e-5},\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(params_to_update, weight_decay=5e-4)        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "        trainer = MultimodalTrainer(model=model, device=device, optimizer=optimizer, \n",
    "                                  criterion=criterion, scheduler=scheduler)\n",
    "        checkpoint_dir = f\"checkpoints_ablation/{mode_name}\"\n",
    "\n",
    "        best_val_metrics = trainer.train(train_loader, val_loader, num_epochs=num_epochs, \n",
    "                                       patience=patience, checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "        test_metrics = compute_metrics(model, test_loader, device)\n",
    "        results[mode_name] = {\"test\": test_metrics, \"val\": best_val_metrics}\n",
    "\n",
    "    print(\"\\n=== Ablation Table (Test Set) ===\")\n",
    "    print(f\"{'Mode':<20} | {'Acc':<6} | {'F1':<6} | {'Prec':<6} | {'Recall':<6}\")\n",
    "    print(\"-\" * 60)\n",
    "    for mode, metrics in results.items():\n",
    "        m = metrics[\"test\"]\n",
    "        print(f\"{mode:<20} | {m['acc']:.4f} | {m['f1']:.4f} | {m['precision']:.4f} | {m['recall']:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ============================= RUN =============================\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "results = run_ablation(MultimodalModel, train_loader, val_loader, test_loader,\n",
    "                       device=device, num_classes=3, num_epochs=25, hidden_dim=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09113edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Metrics:\n",
      "Acc: 0.8099, Prec: 0.8174, Rec: 0.8099, F1: 0.8096\n",
      "Total misclassified samples: 96\n",
      "Saved misclassified samples to misclassified_samples.csv\n",
      "\n",
      "Example misclassified samples (first 5):\n",
      "   sample_index  pred  label\n",
      "0             0     0      2\n",
      "1            14     1      2\n",
      "2            23     1      2\n",
      "3            25     0      1\n",
      "4            26     0      1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import logging\n",
    "\n",
    "# ============================== LOGGING =============================\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"peft\").setLevel(logging.ERROR)\n",
    "\n",
    "# ============================== DEVICE ==============================\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# ===================== LOAD DATASET (TEST LOADER) ===================\n",
    "test_loader = create_dataloader(data_test, labels_test, batch_size=16, shuffle=False)\n",
    "\n",
    "# ================ DEFINE MODEL CREATION FUNCTION ===================\n",
    "def create_multimodal_model():\n",
    "    return MultimodalModel(\n",
    "        video_dim=256,\n",
    "        text_dim=256,\n",
    "        caption_dim=256,\n",
    "        hidden_dim=256,\n",
    "        num_classes=3,\n",
    "        r=6\n",
    "    )\n",
    "\n",
    "# ==================== ENSEMBLE PREDICTION FUNCTION ===================\n",
    "def ensemble_predict(batch, models, device):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            output = model(batch['video_feat'], batch['text_feat'], batch['caption_feat'])\n",
    "            probs.append(torch.softmax(output, dim=1))\n",
    "    avg_probs = torch.stack(probs).mean(dim=0)\n",
    "    return avg_probs.argmax(dim=1)\n",
    "\n",
    "# ======================= LOAD 3-fold MODELS ==========================\n",
    "model_paths = [\n",
    "    \"/Users/jumita/Downloads/checkpoints_fold_1/best_model.pth\",\n",
    "    \"/Users/jumita/Downloads/checkpoints_fold_2/best_model.pth\",\n",
    "    \"/Users/jumita/Downloads/checkpoints_fold_3/best_model.pth\"\n",
    "]\n",
    "\n",
    "fold_models = []\n",
    "for path in model_paths:\n",
    "    model = create_multimodal_model().to(device)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    fold_models.append(model)\n",
    "\n",
    "# ======================== RUN ERROR ANALYSIS ==========================\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "misclassified = []\n",
    "\n",
    "sample_index = 0  # keep track of global index\n",
    "for batch in test_loader:\n",
    "    labels = batch['label'].numpy()\n",
    "    preds = ensemble_predict(batch, fold_models, device).cpu().numpy()\n",
    "    \n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels)\n",
    "    \n",
    "    for i, (p, l) in enumerate(zip(preds, labels)):\n",
    "        if p != l:\n",
    "            misclassified.append({\n",
    "                'sample_index': sample_index,\n",
    "                'pred': int(p),\n",
    "                'label': int(l)\n",
    "            })\n",
    "        sample_index += 1\n",
    "\n",
    "# ========================== COMPUTE METRICS ===========================\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "prec = precision_score(all_labels, all_preds, average='weighted')\n",
    "rec = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Test Set Metrics:\\nAcc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "print(f\"Total misclassified samples: {len(misclassified)}\")\n",
    "\n",
    "# ======================= SAVE MISCLASSIFIED ===========================\n",
    "df_misclassified = pd.DataFrame(misclassified)\n",
    "df_misclassified.to_csv(\"misclassified_samples.csv\", index=False)\n",
    "print(\"Saved misclassified samples to misclassified_samples.csv\")\n",
    "\n",
    "# ================ Example misclassified samples ========================\n",
    "print(\"\\nExample misclassified samples (first 5):\")\n",
    "print(df_misclassified.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e225222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/mh4k3gks5ps1hxj4j0qrsd9r0000gn/T/ipykernel_82191/3843102682.py:22: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=labels_pairs, y=counts_pairs, ax=axes[0], palette=\"coolwarm\")\n",
      "/var/folders/8g/mh4k3gks5ps1hxj4j0qrsd9r0000gn/T/ipykernel_82191/3843102682.py:29: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=classes, y=counts_ordered, ax=axes[1], palette=\"magma\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# ==================== PREP DATA ====================\n",
    "# Error distribution: True  Pred\n",
    "error_pairs = [(m['label'], m['pred']) for m in misclassified]\n",
    "pair_counts = Counter(error_pairs)\n",
    "labels_pairs = [f\"{l}->{p}\" for (l, p) in pair_counts.keys()]\n",
    "counts_pairs = [count for count in pair_counts.values()]\n",
    "\n",
    "# Misclassified count per true class\n",
    "true_labels = [m['label'] for m in misclassified]\n",
    "label_counts = Counter(true_labels)\n",
    "classes = sorted(set(true_labels))\n",
    "counts_ordered = [label_counts.get(c, 0) for c in classes]\n",
    "\n",
    "# ==================== PLOT =========================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ----- Left: Error Distribution -----\n",
    "sns.barplot(x=labels_pairs, y=counts_pairs, ax=axes[0], palette=\"coolwarm\")\n",
    "axes[0].set_title(\"Error Distribution (True  Pred)\", fontsize=14)\n",
    "axes[0].set_xlabel(\"True  Predicted\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Count\", fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ----- Right: Misclassified Count per True Class -----\n",
    "sns.barplot(x=classes, y=counts_ordered, ax=axes[1], palette=\"magma\")\n",
    "axes[1].set_title(\"Misclassified Samples per True Class\", fontsize=14)\n",
    "axes[1].set_xlabel(\"True Class\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Number of Misclassifications\", fontsize=12)\n",
    "\n",
    "# Thm s count trn ct\n",
    "for ax, counts in zip([axes[0], axes[1]], [counts_pairs, counts_ordered]):\n",
    "    for idx, val in enumerate(counts):\n",
    "        ax.text(idx, val + 0.5, str(val), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"misclassified_analysis_combined.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd-webui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
