{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9642a74",
   "metadata": {},
   "source": [
    "Frame process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76119a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frame features: 100%|██████████| 39/39 [01:22<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config Saved] /Users/jumitatran/Downloads/not_in_excel/frame/feature_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "def extract_frames_from_video(video_path, frame_interval=30, resize=(224, 224)):\n",
    "    \"\"\"\n",
    "    Extract frames from a video at a fixed interval and resize them.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        frame_interval (int): Interval in frames for extraction (e.g., 30 = one frame every ~1s at 30 FPS).\n",
    "        resize (tuple): Desired output size (width, height).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            frames (list[PIL.Image]): List of extracted frames as PIL Images.\n",
    "            fps (float): Frames per second of the original video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_interval == 0:\n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Resize frame to (224,224)\n",
    "            frame_resized = cv2.resize(frame_rgb, resize)\n",
    "            pil_img = Image.fromarray(frame_resized)\n",
    "            frames.append(pil_img)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    return frames, fps\n",
    "\n",
    "\n",
    "class VideoFeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading pre-extracted video frame features from .npy files.\n",
    "\n",
    "    Each .npy file contains a tensor of shape (num_frames, num_tokens, feature_dim).\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_dir (str): Directory containing .npy feature files.\n",
    "        \"\"\"\n",
    "        self.feature_dir = feature_dir\n",
    "        self.feature_files = [f for f in os.listdir(feature_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load features for the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the feature file.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                features (Tensor): Extracted features as a float tensor.\n",
    "                video_id (str): Name of the video (file name without extension).\n",
    "        \"\"\"\n",
    "        file_name = self.feature_files[idx]\n",
    "        path = os.path.join(self.feature_dir, file_name)\n",
    "        features = np.load(path)\n",
    "        features = torch.from_numpy(features).float()\n",
    "        video_id = os.path.splitext(file_name)[0]\n",
    "        return features, video_id\n",
    "\n",
    "\n",
    "def setup_model(device='cpu'):\n",
    "    \"\"\"\n",
    "    Initialize the Vision Transformer (ViT) feature extractor and projection layer.\n",
    "\n",
    "    Args:\n",
    "        device (str): Device to place model on ('cpu', 'cuda', or 'mps').\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            vit_model (ViTModel): Pretrained ViT model.\n",
    "            f_fc (torch.nn.Linear): Linear projection layer (768 -> 256).\n",
    "            processor (ViTImageProcessor): Image processor for preprocessing frames.\n",
    "    \"\"\"\n",
    "    model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "    processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "    vit_model = ViTModel.from_pretrained(model_name)\n",
    "    vit_model.to(device)\n",
    "    f_fc = torch.nn.Linear(768, 256).to(device)\n",
    "    return vit_model, f_fc, processor\n",
    "\n",
    "\n",
    "def extract_256d_features_tokenwise(vit_model, f_fc, processor, images, device):\n",
    "    \"\"\"\n",
    "    Extract token-wise features and project them to 256-dim using a linear layer.\n",
    "\n",
    "    Args:\n",
    "        vit_model (ViTModel): Pretrained ViT model.\n",
    "        f_fc (torch.nn.Linear): Projection layer (768 -> 256).\n",
    "        processor (ViTImageProcessor): Preprocessor for ViT.\n",
    "        images (list[PIL.Image]): List of frames to process.\n",
    "        device (str): Device for inference.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (num_frames, num_tokens, 256).\n",
    "    \"\"\"\n",
    "    vit_model.eval()\n",
    "    f_fc.eval()\n",
    "    all_features = []\n",
    "    batch_size = 32\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch_imgs = images[i:i+batch_size]\n",
    "            inputs = processor(images=batch_imgs, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = vit_model(**inputs)\n",
    "            token_embeddings = outputs.last_hidden_state # (B, num_tokens, 768)\n",
    "            features_256d = f_fc(token_embeddings)       # Project to 256D\n",
    "            all_features.append(features_256d.cpu())\n",
    "    return torch.cat(all_features, dim=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # Paths for input videos and output embeddings\n",
    "    video_folder = \"/Users/jumita/Downloads/Book5\"\n",
    "    frame_feature_dir = \"/Users/jumita/Downloads/final/frame\"\n",
    "    os.makedirs(frame_feature_dir, exist_ok=True)\n",
    "\n",
    "    video_extensions = [\".mp4\", \".avi\", \".mov\", \".mkv\"]\n",
    "    video_paths = [\n",
    "        os.path.join(video_folder, f)\n",
    "        for f in os.listdir(video_folder)\n",
    "        if os.path.isfile(os.path.join(video_folder, f)) and os.path.splitext(f)[1].lower() in video_extensions\n",
    "    ]\n",
    "\n",
    "    vit_model, f_fc, processor = setup_model(device=device)\n",
    "\n",
    "    for video_path in tqdm(video_paths, desc=\"Extracting frame features\"):\n",
    "        video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        frames, fps = extract_frames_from_video(video_path, frame_interval=30)\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            print(f\"[Warning] No frames extracted from {video_name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        features = extract_256d_features_tokenwise(vit_model, f_fc, processor, frames, device)\n",
    "\n",
    "        # Save extracted features to .npy\n",
    "        save_feat_path = os.path.join(frame_feature_dir, f\"{video_name}.npy\")\n",
    "        np.save(save_feat_path, features.numpy())\n",
    "\n",
    "    # Save config info for reference\n",
    "    config = {\n",
    "        \"model_name\": \"google/vit-base-patch16-224-in21k\",\n",
    "        \"feature_dim\": 256,\n",
    "        \"frame_interval\": 30,\n",
    "        \"device\": device,\n",
    "        \"num_videos\": len(video_paths)\n",
    "    }\n",
    "    config_path = os.path.join(frame_feature_dir, \"feature_config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    print(f\"[Config Saved] {config_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aabb907",
   "metadata": {},
   "source": [
    "OCR and sound - extract, fine tune and embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f68ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 video files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 17912/17912 [00:13<00:00, 1288.64frames/s]\n",
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 10%|█         | 1/10 [03:09<28:28, 189.84s/it]/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 2080/2080 [00:11<00:00, 189.04frames/s]\n",
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 20%|██        | 2/10 [03:44<13:07, 98.49s/it] /opt/anaconda3/envs/newenv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 9209/9209 [00:08<00:00, 1098.20frames/s]\n",
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 30%|███       | 3/10 [04:22<08:16, 70.90s/it]/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 39%|███▉      | 1950/4950 [00:01<00:02, 1240.89frames/s]\n",
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 40%|████      | 4/10 [04:42<05:05, 50.99s/it]/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "  0%|          | 0/831 [00:00<?, ?frames/s]\n",
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 50%|█████     | 5/10 [04:54<03:04, 36.84s/it]/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 766/766 [00:00<00:00, 898.24frames/s]\n",
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 60%|██████    | 6/10 [05:09<01:57, 29.39s/it]/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 16941/16941 [00:08<00:00, 1931.86frames/s]\n",
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 70%|███████   | 7/10 [08:03<03:50, 76.75s/it]/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 3692/3692 [00:13<00:00, 264.87frames/s]\n",
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 80%|████████  | 8/10 [08:56<02:18, 69.12s/it]/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 487/487 [00:00<00:00, 706.55frames/s]\n",
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 90%|█████████ | 9/10 [09:00<00:48, 48.84s/it]/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "100%|██████████| 1272/1272 [00:01<00:00, 1149.54frames/s]\n",
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "100%|██████████| 10/10 [09:16<00:00, 55.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted texts for 10 videos at /Users/jumitatran/Downloads/extracted_texts2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/newenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embedding for 7504644259056831766 at /Users/jumitatran/Downloads/not_in_excel/text/7504644259056831766.npy\n",
      "Saved embedding for 7496083643278822699 at /Users/jumitatran/Downloads/not_in_excel/text/7496083643278822699.npy\n",
      "Saved embedding for 7520700854324694294 at /Users/jumitatran/Downloads/not_in_excel/text/7520700854324694294.npy\n",
      "Saved embedding for 7515801813078101279 at /Users/jumitatran/Downloads/not_in_excel/text/7515801813078101279.npy\n",
      "Saved embedding for 7509974643906710806 at /Users/jumitatran/Downloads/not_in_excel/text/7509974643906710806.npy\n",
      "Saved embedding for 7517408801704643895 at /Users/jumitatran/Downloads/not_in_excel/text/7517408801704643895.npy\n",
      "Saved embedding for 7496217736339541270 at /Users/jumitatran/Downloads/not_in_excel/text/7496217736339541270.npy\n",
      "Saved embedding for 7522994842775850270 at /Users/jumitatran/Downloads/not_in_excel/text/7522994842775850270.npy\n",
      "Saved embedding for 7518306957858475282 at /Users/jumitatran/Downloads/not_in_excel/text/7518306957858475282.npy\n",
      "Saved embedding for 7512304646275353886 at /Users/jumitatran/Downloads/not_in_excel/text/7512304646275353886.npy\n",
      "Saved 10 embeddings to folder: /Users/jumitatran/Downloads/not_in_excel/text\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import whisper\n",
    "import easyocr\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ===== NLP normalization =====\n",
    "nltk.download(\"words\", quiet=True)\n",
    "english_vocab = set(w.lower() for w in words.words())\n",
    "\n",
    "def normalize(word):\n",
    "    \"\"\"\n",
    "    Normalize a word by:\n",
    "    - Removing non-alphanumeric characters.\n",
    "    - Converting to lowercase.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\W+', '', word).lower()\n",
    "\n",
    "def is_english_word(word):\n",
    "    \"\"\"\n",
    "    Check if a word is an English word or a number.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if word is in NLTK's English vocabulary or is numeric.\n",
    "    \"\"\"\n",
    "    norm = normalize(word)\n",
    "    return norm.isdigit() or norm in english_vocab\n",
    "\n",
    "def clean_and_filter_english(text):\n",
    "    \"\"\"\n",
    "    Clean a text string by filtering out non-English words.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Filtered string containing only English words and numbers.\n",
    "    \"\"\"\n",
    "    word_list = text.strip().split()\n",
    "    return \" \".join([w for w in word_list if is_english_word(w)])\n",
    "\n",
    "# ===== OCR from image =====\n",
    "def extract_ocr_text(frame, reader):\n",
    "    \"\"\"\n",
    "    Extract and clean OCR text from an image frame using EasyOCR.\n",
    "\n",
    "    Args:\n",
    "        frame (np.ndarray): Frame image from video (BGR format).\n",
    "        reader (easyocr.Reader): EasyOCR reader object.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned OCR text from the frame.\n",
    "    \"\"\"\n",
    "    results = reader.readtext(frame, detail=1, paragraph=False)\n",
    "    filtered = []\n",
    "    for bbox, text, conf in results:\n",
    "        text = clean_and_filter_english(text)\n",
    "        if text:\n",
    "            filtered.append(text)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "# ===== ASR transcript lookup by timestamp =====\n",
    "def find_transcript_for_time(segments, timestamp):\n",
    "    \"\"\"\n",
    "    Find ASR transcript text for a given timestamp by searching Whisper's output segments.\n",
    "\n",
    "    Args:\n",
    "        segments (list): List of Whisper transcription segments with 'start', 'end', 'text'.\n",
    "        timestamp (float): Timestamp (seconds) to look up.\n",
    "\n",
    "    Returns:\n",
    "        str: Transcript text for the given time (empty string if not found).\n",
    "    \"\"\"\n",
    "    for segment in segments:\n",
    "        if segment['start'] <= timestamp <= segment['end']:\n",
    "            return segment['text']\n",
    "    return \"\"\n",
    "\n",
    "# ===== Extract text from video =====\n",
    "def extract_texts_from_video(video_path, whisper_model, reader, fps_interval=5):\n",
    "    \"\"\"\n",
    "    Extract combined text from video using:\n",
    "    - Whisper ASR for audio transcription.\n",
    "    - EasyOCR for text in frames.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        whisper_model: Whisper model for ASR.\n",
    "        reader: EasyOCR reader object.\n",
    "        fps_interval (int): Interval in seconds for OCR frame sampling.\n",
    "\n",
    "    Returns:\n",
    "        str: Combined (OCR + ASR) text extracted from video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    interval = int(fps * fps_interval) if fps > 0 else 1\n",
    "    frame_id = 0\n",
    "\n",
    "    results = whisper_model.transcribe(video_path, language=\"en\", verbose=False)\n",
    "    segments = results.get(\"segments\", [])\n",
    "    all_texts = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_id % interval == 0:\n",
    "            ocr_text = extract_ocr_text(frame, reader)\n",
    "            timestamp = frame_id / fps if fps > 0 else 0\n",
    "            asr_text = find_transcript_for_time(segments, timestamp)\n",
    "            asr_text = clean_and_filter_english(asr_text)\n",
    "            ocr_text = clean_and_filter_english(ocr_text)\n",
    "\n",
    "            combined_text = (asr_text + \" \" + ocr_text).strip()\n",
    "            if combined_text:\n",
    "                all_texts.append(combined_text)\n",
    "        frame_id += 1\n",
    "\n",
    "    cap.release()\n",
    "    merged_text = \" \".join(all_texts).strip()\n",
    "    return merged_text\n",
    "\n",
    "# ===== Text embedding model with projection =====\n",
    "class BERTEmbeddingWithProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT-based text embedding extractor with linear projection to 256D.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.base = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.proj = nn.Linear(self.base.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        out = self.base(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        cls = out.last_hidden_state[:, 0, :]\n",
    "        return self.proj(cls)\n",
    "\n",
    "def create_and_save_text_embeddings(texts, video_ids, model, tokenizer, device=\"cpu\", batch_size=8, save_dir=\"./\"):\n",
    "    \"\"\"\n",
    "    Create and save text embeddings for a list of texts.\n",
    "\n",
    "    Args:\n",
    "        texts (list[str]): List of extracted texts.\n",
    "        video_ids (list[str]): Corresponding video IDs.\n",
    "        model (nn.Module): Text embedding model.\n",
    "        tokenizer: HuggingFace tokenizer for input encoding.\n",
    "        device (str): Device to run inference on (\"cpu\", \"cuda\", \"mps\").\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        save_dir (str): Directory to save .npy embeddings.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_ids = video_ids[i:i+batch_size]\n",
    "\n",
    "        enc = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc)\n",
    "            embeddings = out.cpu().numpy()\n",
    "\n",
    "        for vid, emb in zip(batch_ids, embeddings):\n",
    "            save_path = os.path.join(save_dir, f\"{vid}.npy\")\n",
    "            np.save(save_path, emb)\n",
    "            print(f\"Saved embedding for {vid} at {save_path}\")\n",
    "\n",
    "# ===== Main execution =====\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/Users/jumitatran/Downloads/Book5\"\n",
    "    device = \"cpu\"\n",
    "\n",
    "    whisper_model = whisper.load_model(\"small\")\n",
    "    use_gpu_easyocr = (device == \"cuda\")\n",
    "    reader = easyocr.Reader(['en'], gpu=use_gpu_easyocr)\n",
    "\n",
    "    video_files = glob.glob(os.path.join(folder_path, \"*.mp4\"))\n",
    "    print(f\"Found {len(video_files)} video files.\")\n",
    "\n",
    "    all_texts = []\n",
    "    video_ids = []\n",
    "\n",
    "    # Extract text from videos\n",
    "    for video_path in tqdm(video_files):\n",
    "        merged_text = extract_texts_from_video(video_path, whisper_model, reader)\n",
    "        if not merged_text:\n",
    "            merged_text = \"\"\n",
    "        all_texts.append(merged_text)\n",
    "        video_ids.append(os.path.splitext(os.path.basename(video_path))[0])\n",
    "\n",
    "    # Load labels for reference\n",
    "    df = pd.read_excel(\"/Users/jumitatran/Downloads/Book5.xlsx\")\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    # Initialize embedding model\n",
    "    embed_model = BERTEmbeddingWithProjection(output_dim=256).to(device)\n",
    "    embed_model.eval()\n",
    "    embed_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Save extracted text to JSON for inspection\n",
    "    output_text_path = \"/Users/jumitatran/Downloads/extracted_texts2.json\"\n",
    "    with open(output_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dict(zip(video_ids, all_texts)), f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved extracted texts for {len(video_ids)} videos at {output_text_path}\")\n",
    "    # Create embeddings for all texts\n",
    "    create_and_save_text_embeddings(\n",
    "        all_texts,\n",
    "        video_ids,\n",
    "        embed_model,\n",
    "        embed_tokenizer,\n",
    "        device=device,\n",
    "        batch_size=8,\n",
    "        save_dir=\"/Users/jumitatran/Downloads/not_in_excel/text\"\n",
    "    )\n",
    "    print(f\"Saved {len(video_ids)} embeddings to folder: /Users/jumitatran/Downloads/not_in_excel/text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd11b33",
   "metadata": {},
   "source": [
    "Metadata embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd01b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 74 embeddings (256-d) to: /Users/jumita/Downloads/excel/caption\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class TikTokProcessorWithProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    Text embedding processor for TikTok captions using BERT with a projection layer.\n",
    "\n",
    "    This class:\n",
    "    - Loads a pretrained BERT model (default: bert-base-uncased).\n",
    "    - Adds a linear projection layer to reduce embedding dimension (e.g. 768 → 256).\n",
    "    - Encodes a list of texts into projected embeddings.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): HuggingFace model name (default \"bert-base-uncased\").\n",
    "        output_dim (int): Target dimensionality of output embeddings (default 256).\n",
    "        device (str): Device to run the model (\"cpu\" / \"cuda\").\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", output_dim=256, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.proj = nn.Linear(self.bert.config.hidden_size, output_dim)  # 768->256\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, texts, batch_size=32, max_length=256):\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts (list[str]): List of input texts to encode.\n",
    "            batch_size (int): Number of samples per batch for processing.\n",
    "            max_length (int): Max token length for truncation.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Numpy array of shape (len(texts), output_dim).\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i+batch_size]\n",
    "                inputs = self.tokenizer(batch, padding=True, truncation=True,\n",
    "                                        max_length=max_length, return_tensors=\"pt\").to(self.device)\n",
    "                outputs = self.bert(**inputs, return_dict=True)\n",
    "                cls_emb = outputs.last_hidden_state[:, 0, :]  \n",
    "                projected = self.proj(cls_emb)\n",
    "                embeddings.append(projected.cpu().numpy())\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "def load_data_from_excel(excel_path, desc_col='description', id_col='video_id'):\n",
    "    \"\"\"\n",
    "    Load video descriptions and IDs from an Excel file.\n",
    "\n",
    "    Args:\n",
    "        excel_path (str): Path to the Excel file.\n",
    "        desc_col (str): Column name containing video descriptions.\n",
    "        id_col (str): Column name containing video IDs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of descriptions, list of video IDs)\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(excel_path)\n",
    "    df = df.dropna(subset=[desc_col, id_col])\n",
    "    descriptions = df[desc_col].astype(str).tolist()\n",
    "    video_ids = df[id_col].astype(str).tolist()\n",
    "    return descriptions, video_ids\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    excel_path = \"/Users/jumita/Downloads/Book1.xlsx\"\n",
    "    output_dir = \"/Users/jumita/Downloads/excel/caption\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    all_texts, video_ids = load_data_from_excel(excel_path)\n",
    "\n",
    "    # Create model processor\n",
    "    processor = TikTokProcessorWithProjection(model_name=\"bert-base-uncased\", output_dim=256, device=device)\n",
    "\n",
    "    # Compute embeddings with projection layer\n",
    "    reduced_embeddings = processor(all_texts, batch_size=32)\n",
    "\n",
    "    # Save vector 256-dim\n",
    "    for vid, emb in zip(video_ids, reduced_embeddings):\n",
    "        np.save(os.path.join(output_dir, f\"{vid}.npy\"), emb)\n",
    "\n",
    "    print(f\"Saved {len(video_ids)} embeddings (256-d) to: {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd-webui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
