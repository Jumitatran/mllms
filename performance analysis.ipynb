{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ebb284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "sys.path.append(\"/Users/jumita/Downloads/FakingRecipe-main/model/\")\n",
    "\n",
    "# ================= Device =================\n",
    "device = \"mps\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ================= Dataset =================\n",
    "class VideoTextCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading video, text, and caption features\n",
    "    along with their corresponding labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_list (list of dict): Each dict contains paths to video, text, and caption features.\n",
    "            labels (array-like): Labels corresponding to each data sample.\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.labels = labels\n",
    "\n",
    "    def process_feat(self, feat):\n",
    "        \"\"\"\n",
    "        Process a feature array by flattening or averaging depending on its shape.\n",
    "\n",
    "        Args:\n",
    "            feat (np.ndarray): Input feature array of shape (256,), (seq, 256), or (seq, 197, 256).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed 1D feature tensor of size 256.\n",
    "        \"\"\"\n",
    "        feat = np.array(feat)\n",
    "        if feat.ndim == 1:  # (256,)\n",
    "            return torch.from_numpy(feat).float()\n",
    "        elif feat.ndim == 2:  # (seq, 256)\n",
    "            return torch.from_numpy(feat.mean(axis=0)).float()\n",
    "        elif feat.ndim == 3:  # (seq, 197, 256)\n",
    "            seq_len = feat.shape[0] * feat.shape[1]\n",
    "            feat = feat.reshape(seq_len, feat.shape[2])\n",
    "            return torch.from_numpy(feat.mean(axis=0)).float()\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected feature shape: {feat.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the features and label of a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (video_feat, text_feat, caption_feat, label) as tensors.\n",
    "        \"\"\"\n",
    "        item = self.data_list[idx]\n",
    "        video_feat = self.process_feat(np.load(item[\"video_feat\"]))\n",
    "        text_feat = self.process_feat(np.load(item[\"text_feat\"]))\n",
    "        caption_feat = self.process_feat(np.load(item[\"caption_feat\"]))\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return video_feat, text_feat, caption_feat, label\n",
    "\n",
    "# ================= Load or Create JSON =================\n",
    "def load_or_create_data(json_path, excel_path, labels_path, video_dir, text_dir, caption_dir):\n",
    "    \"\"\"\n",
    "    Load data list from JSON or create it from Excel and feature directories.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to JSON file storing data list.\n",
    "        excel_path (str): Path to Excel file containing video IDs.\n",
    "        labels_path (str): Path to .npy file containing labels.\n",
    "        video_dir (str): Directory containing video feature .npy files.\n",
    "        text_dir (str): Directory containing text feature .npy files.\n",
    "        caption_dir (str): Directory containing caption feature .npy files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (data_list, labels)\n",
    "    \"\"\"\n",
    "    labels = np.load(labels_path)\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data_list = json.load(f)\n",
    "    else:\n",
    "        df = pd.read_excel(excel_path)\n",
    "        data_list = []\n",
    "        for vid in df['video_id'].astype(str):\n",
    "            data_list.append({\n",
    "                \"video_feat\": os.path.join(video_dir, f\"{vid}.npy\"),\n",
    "                \"text_feat\": os.path.join(text_dir, f\"{vid}.npy\"),\n",
    "                \"caption_feat\": os.path.join(caption_dir, f\"{vid}.npy\")\n",
    "            })\n",
    "        os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(data_list, f)\n",
    "    return data_list, labels\n",
    "\n",
    "# ================= Paths =================\n",
    "json_path    = \"/Users/jumita/Downloads/final_code/data.json\"\n",
    "excel_path   = \"/Users/jumita/Downloads/Book5.xlsx\"\n",
    "labels_path  = \"/Users/jumita/Downloads/final_code/labels.npy\"\n",
    "video_dir    = \"/Users/jumita/Downloads/final/frame\"\n",
    "text_dir     = \"/Users/jumita/Downloads/final/text\"\n",
    "caption_dir  = \"/Users/jumita/Downloads/final/caption\"\n",
    "\n",
    "data_list, labels = load_or_create_data(json_path, excel_path, labels_path, video_dir, text_dir, caption_dir)\n",
    "\n",
    "# ================= Remove duplicates =================\n",
    "def remove_duplicates(data_list, labels):\n",
    "    \"\"\"\n",
    "    Remove duplicate video entries to avoid repeated data.\n",
    "\n",
    "    Args:\n",
    "        data_list (list of dict): List containing feature paths.\n",
    "        labels (np.ndarray): Corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (unique_data_list, unique_labels)\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    unique_data = []\n",
    "    unique_labels = []\n",
    "    for item, label in zip(data_list, labels):\n",
    "        if item[\"video_feat\"] not in seen:\n",
    "            seen.add(item[\"video_feat\"])\n",
    "            unique_data.append(item)\n",
    "            unique_labels.append(label)\n",
    "    return unique_data, np.array(unique_labels)\n",
    "\n",
    "data_list, labels = remove_duplicates(data_list, labels)\n",
    "\n",
    "# ================= Split =================\n",
    "data_train, data_temp, labels_train, labels_temp = train_test_split(\n",
    "    data_list, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "try:\n",
    "    data_val, data_test, labels_val, labels_test = train_test_split(\n",
    "        data_temp, labels_temp, test_size=0.5, random_state=42, stratify=labels_temp\n",
    "    )\n",
    "except ValueError:\n",
    "    data_val, data_test, labels_val, labels_test = train_test_split(\n",
    "        data_temp, labels_temp, test_size=0.5, random_state=42, stratify=None\n",
    "    )\n",
    "\n",
    "# ================= DataLoader =================\n",
    "train_loader = DataLoader(VideoTextCaptionDataset(data_train, labels_train), batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(VideoTextCaptionDataset(data_val, labels_val), batch_size=16, shuffle=False)\n",
    "test_loader  = DataLoader(VideoTextCaptionDataset(data_test, labels_test), batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9971bbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap train-val: 0\n",
      "Overlap train-test: 0\n",
      "Overlap val-test: 0\n",
      "Augmented sample shapes:\n",
      "Video: torch.Size([16, 256])\n",
      "Text: torch.Size([16, 256])\n",
      "Caption: torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ================= VIDEO FEATURE AUGMENTER =================\n",
    "class VideoFeatureAugmenter:\n",
    "    \"\"\"\n",
    "    Data augmenter for video features.\n",
    "\n",
    "    This class applies temporal cropping and adaptive Gaussian noise \n",
    "    to video features to improve model generalization during training.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    augment_prob : float\n",
    "        Probability of applying augmentation.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer used for augmentation (optional, not used here).\n",
    "    noise_scale : float\n",
    "        Scaling factor for Gaussian noise.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    temporal_crop(features, crop_ratio=0.9):\n",
    "        Randomly crops the sequence along the temporal dimension.\n",
    "    adaptive_noise(features):\n",
    "        Adds Gaussian noise proportional to the feature's standard deviation.\n",
    "    __call__(features):\n",
    "        Applies augmentation with probability `augment_prob`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, augment_prob=0.5, dropout_prob=0.3, noise_scale=0.05):\n",
    "        self.augment_prob = augment_prob\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "    def temporal_crop(self, features, crop_ratio=0.9):\n",
    "        \"\"\"\n",
    "        Crop the feature sequence randomly along the temporal dimension.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Video feature tensor of shape (T, D) or (T, H, D).\n",
    "        crop_ratio : float\n",
    "            Fraction of the sequence to keep.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Cropped feature tensor.\n",
    "        \"\"\"\n",
    "        T = features.shape[0]\n",
    "        new_T = int(T * crop_ratio)\n",
    "        start = np.random.randint(0, T - new_T + 1)\n",
    "        return features[start:start+new_T]\n",
    "\n",
    "    def adaptive_noise(self, features):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise scaled by feature standard deviation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Noisy feature tensor.\n",
    "        \"\"\"\n",
    "        std_per_feature = features.std(dim=0, keepdim=True) + 1e-6\n",
    "        noise = torch.randn_like(features) * std_per_feature * self.noise_scale\n",
    "        return features + noise\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Apply augmentation with probability `augment_prob`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Input feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Augmented feature tensor.\n",
    "        \"\"\"\n",
    "        if torch.rand(1) < self.augment_prob:\n",
    "            if features.ndim >= 2:\n",
    "                features = self.temporal_crop(features)\n",
    "            features = self.adaptive_noise(features)\n",
    "        return features\n",
    "\n",
    "# ================= TEXT/CAPTION FEATURE AUGMENTER =================\n",
    "class TextFeatureAugmenter:\n",
    "    \"\"\"\n",
    "    Data augmenter for text or caption features.\n",
    "\n",
    "    Applies dropout and adaptive Gaussian noise to embeddings.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    augment_prob : float\n",
    "        Probability of applying augmentation.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer applied to features.\n",
    "    noise_scale : float\n",
    "        Scaling factor for Gaussian noise.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    adaptive_noise(features):\n",
    "        Adds Gaussian noise proportional to feature's std.\n",
    "    __call__(features):\n",
    "        Applies augmentation with probability `augment_prob`.\n",
    "    \"\"\"\n",
    "    def __init__(self, augment_prob=0.3, dropout_prob=0.05, noise_scale=0.02):\n",
    "        self.augment_prob = augment_prob\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "    def adaptive_noise(self, features):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise scaled by feature standard deviation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Input feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Noisy feature tensor.\n",
    "        \"\"\"\n",
    "        std_per_feature = features.std(dim=0, keepdim=True) + 1e-6\n",
    "        noise = torch.randn_like(features) * std_per_feature * self.noise_scale\n",
    "        return features + noise\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Apply dropout and noise augmentation with probability `augment_prob`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Input feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Augmented feature tensor.\n",
    "        \"\"\"\n",
    "        if torch.rand(1) < self.augment_prob:\n",
    "            features = self.dropout(features)\n",
    "            features = self.adaptive_noise(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "# ================= DATASET =================\n",
    "class VideoTextCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for multimodal video, text, and caption features.\n",
    "\n",
    "    Handles optional data augmentation and per-sample feature normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_list : list of dict\n",
    "        List of dictionaries containing paths for 'video_feat', 'text_feat', 'caption_feat'.\n",
    "    labels : np.ndarray\n",
    "        Array of integer labels corresponding to each sample.\n",
    "    augment : bool, optional\n",
    "        Whether to apply data augmentation (default: False).\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(idx):\n",
    "        Loads, optionally augments, and returns a single sample.\n",
    "    __len__():\n",
    "        Returns the total number of samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, labels, augment=False):\n",
    "        self.data_list = data_list\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        self.video_aug = VideoFeatureAugmenter() if augment else None\n",
    "        self.text_aug = TextFeatureAugmenter() if augment else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data_list[idx]\n",
    "\n",
    "        video_feat = torch.from_numpy(np.load(item[\"video_feat\"])).float()\n",
    "        text_feat = torch.from_numpy(np.load(item[\"text_feat\"])).float()\n",
    "        caption_feat = torch.from_numpy(np.load(item[\"caption_feat\"])).float()\n",
    "\n",
    "        if self.augment:\n",
    "            if self.video_aug and video_feat.ndim >= 2:\n",
    "                video_feat = self.video_aug(video_feat)\n",
    "            if self.text_aug:\n",
    "                text_feat = self.text_aug(text_feat)\n",
    "                caption_feat = self.text_aug(caption_feat)\n",
    "\n",
    "        # Flatten\n",
    "        if video_feat.ndim == 3:\n",
    "            video_feat = video_feat.mean(dim=(0,1))\n",
    "        elif video_feat.ndim == 2:\n",
    "            video_feat = video_feat.mean(dim=0)\n",
    "        \n",
    "        text_feat = text_feat if text_feat.ndim == 1 else text_feat.mean(dim=0)\n",
    "        caption_feat = caption_feat if caption_feat.ndim == 1 else caption_feat.mean(dim=0)\n",
    "\n",
    "        # Normalize\n",
    "        video_feat = (video_feat - video_feat.mean()) / (video_feat.std() + 1e-6)\n",
    "        text_feat = (text_feat - text_feat.mean()) / (text_feat.std() + 1e-6)\n",
    "        caption_feat = (caption_feat - caption_feat.mean()) / (caption_feat.std() + 1e-6)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"video_feat\": video_feat,\n",
    "            \"text_feat\": text_feat,\n",
    "            \"caption_feat\": caption_feat,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "def create_dataloader(data_list, labels, batch_size=16, shuffle=True, augment=False):\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader for the multimodal dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_list : list of dict\n",
    "        List of dictionaries containing feature file paths.\n",
    "    labels : np.ndarray\n",
    "        Array of labels.\n",
    "    batch_size : int, optional\n",
    "        Batch size (default: 16).\n",
    "    shuffle : bool, optional\n",
    "        Whether to shuffle data (default: True).\n",
    "    augment : bool, optional\n",
    "        Whether to apply data augmentation (default: False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataLoader\n",
    "        PyTorch DataLoader for the dataset.\n",
    "    \"\"\"\n",
    "    dataset = VideoTextCaptionDataset(data_list, labels, augment=augment)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# ================= LOAD DATA =================\n",
    "data_list, labels = load_or_create_data(\n",
    "    json_path=\"/Users/jumita/Downloads/final_code/data.json\",\n",
    "    excel_path=\"/Users/jumita/Downloads/Book5.xlsx\",\n",
    "    labels_path=\"/Users/jumita/Downloads/final_code/labels.npy\",\n",
    "    video_dir=\"/Users/jumita/Downloads/final/frame\",\n",
    "    text_dir=\"/Users/jumita/Downloads/final/text\",\n",
    "    caption_dir=\"/Users/jumita/Downloads/final/caption\"\n",
    ")\n",
    "\n",
    "# ================= SPLIT DATA =================\n",
    "data_train, data_temp, labels_train, labels_temp = train_test_split(\n",
    "    data_list, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "try:\n",
    "    data_val, data_test, labels_val, labels_test = train_test_split(\n",
    "        data_temp, labels_temp, test_size=0.5, random_state=42, stratify=labels_temp\n",
    "    )\n",
    "except ValueError:\n",
    "    data_val, data_test, labels_val, labels_test = train_test_split(\n",
    "        data_temp, labels_temp, test_size=0.5, random_state=42, stratify=None\n",
    "    )\n",
    "\n",
    "def check_overlap(set1, set2):\n",
    "    s1 = set([item[\"video_feat\"] for item in set1])\n",
    "    s2 = set([item[\"video_feat\"] for item in set2])\n",
    "    return len(s1 & s2)\n",
    "\n",
    "print(\"Overlap train-val:\", check_overlap(data_train, data_val))\n",
    "print(\"Overlap train-test:\", check_overlap(data_train, data_test))\n",
    "print(\"Overlap val-test:\", check_overlap(data_val, data_test))\n",
    "\n",
    "\n",
    "# ================= DATALOADERS =================\n",
    "train_loader = create_dataloader(data_train, labels_train, batch_size=16, shuffle=True, augment=True)\n",
    "val_loader = create_dataloader(data_val, labels_val, batch_size=16, shuffle=False)\n",
    "test_loader = create_dataloader(data_test, labels_test, batch_size=16, shuffle=False)\n",
    "\n",
    "# ================= DEBUG SAMPLE =================\n",
    "sample = next(iter(train_loader))\n",
    "print(\"Augmented sample shapes:\")\n",
    "print(\"Video:\", sample[\"video_feat\"].shape)\n",
    "print(\"Text:\", sample[\"text_feat\"].shape)\n",
    "print(\"Caption:\", sample[\"caption_feat\"].shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ================= StochasticDepth =================\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Stochastic Depth (also called DropPath) regularization.\n",
    "\n",
    "    During training, randomly drops entire residual paths with probability `drop_prob`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    drop_prob : float\n",
    "        Probability of dropping a path.\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with stochastic depth.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, ...).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor, either dropped or scaled.\n",
    "        \"\"\"\n",
    "        if not self.training or self.drop_prob == 0.0:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        mask = (torch.rand(x.shape[0], device=x.device) < keep_prob).to(x.dtype)\n",
    "        mask = mask.view(-1, *([1] * (x.dim() - 1)))\n",
    "        return x * mask / keep_prob\n",
    "\n",
    "# ================= LoRA Linear =================\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with Low-Rank Adaptation (LoRA).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Input feature dimension.\n",
    "    out_features : int\n",
    "        Output feature dimension.\n",
    "    r : int\n",
    "        Rank of the LoRA adaptation matrices.\n",
    "    alpha : int\n",
    "        Scaling factor for LoRA updates.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, r=6, alpha=6):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / max(1, r)\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(torch.randn(r, in_features) * 1e-3)\n",
    "            self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through LoRA linear layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, in_features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (B, out_features).\n",
    "        \"\"\"\n",
    "        result = self.linear(x)\n",
    "        if self.r > 0:\n",
    "            lora_update = (x @ self.lora_A.T) @ self.lora_B.T\n",
    "            result += lora_update * self.scaling\n",
    "        return result\n",
    "\n",
    "# ================= LoRA MultiheadAttention =================\n",
    "class LoRAMultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention layer with LoRA adaptation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embed_dim : int\n",
    "        Dimension of embeddings.\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    dropout : float\n",
    "        Dropout probability in attention.\n",
    "    batch_first : bool\n",
    "        If True, input shape is (B, S, D).\n",
    "    r : int\n",
    "        LoRA rank.\n",
    "    alpha : int\n",
    "        LoRA scaling factor.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, batch_first=True, r=6, alpha=6):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / max(1, r)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=batch_first)\n",
    "        if r > 0:\n",
    "            self.lora_A_q = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_q = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "            self.lora_A_k = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_k = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "            self.lora_A_v = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_v = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "            self.lora_A_out = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_out = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=False, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through LoRA multihead attention.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query, key, value : torch.Tensor\n",
    "            Input tensors of shape (B, S, D).\n",
    "        key_padding_mask : torch.Tensor, optional\n",
    "            Mask for padded tokens.\n",
    "        need_weights : bool\n",
    "            If True, returns attention weights.\n",
    "        attn_mask : torch.Tensor, optional\n",
    "            Attention mask.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        attn_output : torch.Tensor\n",
    "            Attention output tensor.\n",
    "        attn_weights : torch.Tensor\n",
    "            Attention weights (if `need_weights` is True).\n",
    "        \"\"\"\n",
    "        if self.r > 0:\n",
    "            query = query + (query @ self.lora_A_q.T) @ self.lora_B_q.T * self.scaling\n",
    "            key   = key   + (key   @ self.lora_A_k.T) @ self.lora_B_k.T * self.scaling\n",
    "            value = value + (value @ self.lora_A_v.T) @ self.lora_B_v.T * self.scaling\n",
    "\n",
    "        attn_output, attn_weights = self.attn(\n",
    "            query, key, value,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=need_weights,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "\n",
    "        if self.r > 0:\n",
    "            attn_output = attn_output + (attn_output @ self.lora_A_out.T) @ self.lora_B_out.T * self.scaling\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "# ================= CoAttentionBlock =================\n",
    "class CoAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Co-Attention block between query and key/value features with LoRA and residual feed-forward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_q : int\n",
    "        Dimension of query features.\n",
    "    dim_kv : int\n",
    "        Dimension of key/value features.\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    hidden_dim : int\n",
    "        Hidden dimension for feed-forward network.\n",
    "    dropout : float\n",
    "        Dropout probability.\n",
    "    lora_r : int\n",
    "        LoRA rank.\n",
    "    drop_path_prob : float\n",
    "        Stochastic depth probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_q=256, dim_kv=256, num_heads=8, hidden_dim=256, dropout=0.2, lora_r=6, drop_path_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.query_proj = LoRALinear(dim_q, dim_q, r=lora_r)\n",
    "        self.key_proj   = LoRALinear(dim_kv, dim_q, r=lora_r)\n",
    "        self.value_proj = LoRALinear(dim_kv, dim_q, r=lora_r)\n",
    "        self.attn = LoRAMultiheadAttention(dim_q, num_heads, dropout=dropout, batch_first=True, r=lora_r)\n",
    "        self.gate = nn.Sequential(LoRALinear(dim_q * 2, dim_q, r=lora_r), nn.Sigmoid())\n",
    "        self.ffn = nn.Sequential(\n",
    "            LoRALinear(dim_q, hidden_dim, r=lora_r),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            LoRALinear(hidden_dim, dim_q, r=lora_r),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim_q)\n",
    "        self.norm2 = nn.LayerNorm(dim_q)\n",
    "        self.drop_path = StochasticDepth(drop_path_prob) if drop_path_prob > 0.0 else None\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of CoAttentionBlock.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query, key, value : torch.Tensor\n",
    "            Input feature tensors.\n",
    "        key_padding_mask : torch.Tensor, optional\n",
    "            Mask for padded tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor after co-attention, gating, and feed-forward.\n",
    "        \"\"\"\n",
    "        q = self.query_proj(query)\n",
    "        k = self.key_proj(key)\n",
    "        v = self.value_proj(value)\n",
    "        attn_out, _ = self.attn(q, k, v, key_padding_mask=key_padding_mask)\n",
    "        gate = self.gate(torch.cat([query, attn_out], dim=-1))\n",
    "        out = self.norm1(query + gate * attn_out)\n",
    "        ffn_out = self.ffn(out)\n",
    "        if self.drop_path is not None:\n",
    "            ffn_out = self.drop_path(ffn_out)\n",
    "        return self.norm2(out + ffn_out)\n",
    "\n",
    "# ================= CoAttentionLayer =================\n",
    "class CoAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Co-Attention layer for interaction between video and text features.\n",
    "\n",
    "    It applies co-attention blocks in both directions:\n",
    "    video-to-text and text-to-video, then pools the sequences\n",
    "    and concatenates the outputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_q : int\n",
    "        Dimension of query features (video features).\n",
    "    dim_kv : int\n",
    "        Dimension of key/value features (text features).\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    hidden_dim : int\n",
    "        Hidden dimension for feed-forward networks.\n",
    "    dropout : float\n",
    "        Dropout probability.\n",
    "    lora_r : int\n",
    "        LoRA rank.\n",
    "    max_video_tokens : int\n",
    "        Maximum number of video tokens to keep after pooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_q=256, dim_kv=256, num_heads=8, hidden_dim=256, dropout=0.5, lora_r=6, max_video_tokens=32):\n",
    "        super().__init__()\n",
    "        self.video2text = CoAttentionBlock(dim_q, dim_kv, num_heads, hidden_dim, dropout, lora_r)\n",
    "        self.text2video = CoAttentionBlock(dim_kv, dim_q, num_heads, hidden_dim, dropout, lora_r)\n",
    "        self.max_video_tokens = max_video_tokens\n",
    "\n",
    "    def forward(self, video_feat, text_feat, video_mask=None, text_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of co-attention layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        video_feat : torch.Tensor\n",
    "            Video features, shape (B, T, D) or (B, N, T, D) if patches.\n",
    "        text_feat : torch.Tensor\n",
    "            Text features, shape (B, S, D).\n",
    "        video_mask : torch.Tensor, optional\n",
    "            Mask for video tokens.\n",
    "        text_mask : torch.Tensor, optional\n",
    "            Mask for text tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Concatenated pooled co-attention features, shape (B, D_video + D_text).\n",
    "        \"\"\"\n",
    "        B = video_feat.shape[0]\n",
    "\n",
    "        if video_feat.dim() == 4:  \n",
    "            T, N, D = video_feat.shape[1], video_feat.shape[2], video_feat.shape[3]\n",
    "            video_feat = video_feat.view(B, T * N, D)\n",
    "        elif video_feat.dim() == 2:\n",
    "            video_feat = video_feat.unsqueeze(1)\n",
    "\n",
    "        # Pooling video sequence\n",
    "        if video_feat.shape[1] > self.max_video_tokens:\n",
    "            video_feat = video_feat.transpose(1, 2)  # (B, D, Seq)\n",
    "            video_feat = F.adaptive_avg_pool1d(video_feat, self.max_video_tokens)\n",
    "            video_feat = video_feat.transpose(1, 2)\n",
    "\n",
    "        if text_feat.dim() == 2:\n",
    "            text_feat = text_feat.unsqueeze(1)\n",
    "\n",
    "        # Forward co-attention with optional masks\n",
    "        v2t = self.video2text(video_feat, text_feat, text_feat, key_padding_mask=text_mask)\n",
    "        t2v = self.text2video(text_feat, video_feat, video_feat, key_padding_mask=video_mask)\n",
    "        \n",
    "        # Pool sequences\n",
    "        v2t_pooled = v2t.mean(dim=1)\n",
    "        t2v_pooled = t2v.mean(dim=1)\n",
    "        return torch.cat([v2t_pooled, t2v_pooled], dim=-1)\n",
    "\n",
    "# ================= LateFusionLoRA =================\n",
    "class LateFusionLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Late fusion module with LoRA for combining two feature modalities.\n",
    "\n",
    "    Applies two-layer MLP with LayerNorm, GELU activation, dropout,\n",
    "    and optional stochastic depth, then outputs class logits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_feat1 : int\n",
    "        Dimension of first input feature.\n",
    "    dim_feat2 : int\n",
    "        Dimension of second input feature.\n",
    "    dim_hidden : int\n",
    "        Hidden layer dimension.\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "    r : int\n",
    "        LoRA rank.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_feat1, dim_feat2, dim_hidden, num_classes, r=6):\n",
    "        super().__init__()\n",
    "        self.fc1 = LoRALinear(dim_feat1 + dim_feat2, dim_hidden * 2, r=r)\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden * 2)\n",
    "        self.fc2 = LoRALinear(dim_hidden * 2, dim_hidden, r=r)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        self.fc3 = LoRALinear(dim_hidden, num_classes, r=r)\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        self.drop_path = StochasticDepth(0.2) if dim_hidden > 256 else None\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Forward pass of late fusion module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x1, x2 : torch.Tensor\n",
    "            Input feature tensors to fuse, shape (B, dim_feat1), (B, dim_feat2).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output class logits, shape (B, num_classes).\n",
    "        \"\"\"\n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# ================= MultimodalModel =================\n",
    "class MultimodalModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal model combining co-attention between video and text features\n",
    "    and late fusion with caption features, with LoRA finetuning support.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    video_dim : int\n",
    "        Dimension of video features.\n",
    "    text_dim : int\n",
    "        Dimension of text features.\n",
    "    caption_dim : int\n",
    "        Dimension of caption features.\n",
    "    hidden_dim : int\n",
    "        Hidden dimension for co-attention and fusion layers.\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "    r : int\n",
    "        LoRA rank.\n",
    "    finetune_mode : str\n",
    "        Finetuning mode: 'coattn_plus_latefusion', 'late_fusion_only', 'coattn_only', etc.\n",
    "    dropout_prob : float\n",
    "        Dropout probability for the fusion layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, video_dim=256, text_dim=256, caption_dim=256, hidden_dim=256,\n",
    "                 num_classes=3, r=6, finetune_mode: str = 'coattn_plus_latefusion',\n",
    "                 dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.ca1 = CoAttentionLayer(video_dim, text_dim, num_heads=8, hidden_dim=hidden_dim, lora_r=r)\n",
    "        self.fc_ca1 = LoRALinear(video_dim * 2, video_dim, r=r)\n",
    "        self.dropout = nn.Dropout(dropout_prob)   \n",
    "        self.late_fusion = LateFusionLoRA(video_dim, caption_dim, hidden_dim, num_classes, r=r)\n",
    "        self.set_trainable(finetune_mode)\n",
    "\n",
    "    def set_trainable(self, finetune_mode: str = 'coattn_plus_latefusion'):\n",
    "        \"\"\"\n",
    "        Set which parameters are trainable according to finetune_mode.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        finetune_mode : str\n",
    "            Mode of finetuning.\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        finetune_mode = finetune_mode.lower()\n",
    "\n",
    "        def enable_lora(module):\n",
    "            for name, p in module.named_parameters():\n",
    "                if \"lora_A\" in name or \"lora_B\" in name:\n",
    "                    p.requires_grad = True\n",
    "\n",
    "        def unfreeze_part(module, names_to_unfreeze):\n",
    "            for name, p in module.named_parameters():\n",
    "                for target_name in names_to_unfreeze:\n",
    "                    if target_name in name:\n",
    "                        p.requires_grad = True\n",
    "\n",
    "        if finetune_mode == 'late_fusion_only':\n",
    "            enable_lora(self.late_fusion)\n",
    "            enable_lora(self.fc_ca1)\n",
    "        elif finetune_mode == 'coattn_only':\n",
    "            enable_lora(self.ca1)\n",
    "            enable_lora(self.fc_ca1)\n",
    "        elif finetune_mode == 'coattn_plus_latefusion':\n",
    "            enable_lora(self.ca1)\n",
    "            enable_lora(self.late_fusion)\n",
    "            enable_lora(self.fc_ca1)\n",
    "        elif finetune_mode == 'coattn_plus_latefusion_part':\n",
    "            enable_lora(self.ca1)\n",
    "            enable_lora(self.late_fusion)\n",
    "            enable_lora(self.fc_ca1)\n",
    "            unfreeze_part(self.ca1.video2text.query_proj, ['linear'])\n",
    "            unfreeze_part(self.late_fusion.fc1, ['linear'])\n",
    "        elif finetune_mode == 'all':\n",
    "            for p in self.parameters():\n",
    "                p.requires_grad = True\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown finetune_mode: {finetune_mode}\")\n",
    "\n",
    "    def get_param_groups(self, lr_late=2e-4, lr_coattn=1.5e-4, lr_fc_ca1=5e-5, weight_decay=5e-4):\n",
    "        \"\"\"\n",
    "        Return parameter groups with separate learning rates for optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr_late, lr_coattn, lr_fc_ca1 : float\n",
    "            Learning rates for respective modules.\n",
    "        weight_decay : float\n",
    "            Weight decay for optimizer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of dict\n",
    "            Parameter groups for optimizer.\n",
    "        \"\"\"\n",
    "        groups = []\n",
    "        if any(p.requires_grad for p in self.late_fusion.parameters()):\n",
    "            groups.append({'params': [p for p in self.late_fusion.parameters() if p.requires_grad], 'lr': lr_late, 'weight_decay': weight_decay})\n",
    "        if any(p.requires_grad for p in self.ca1.parameters()):\n",
    "            groups.append({'params': [p for p in self.ca1.parameters() if p.requires_grad], 'lr': lr_coattn, 'weight_decay': weight_decay})\n",
    "        if any(p.requires_grad for p in self.fc_ca1.parameters()):\n",
    "            groups.append({'params': [p for p in self.fc_ca1.parameters() if p.requires_grad], 'lr': lr_fc_ca1, 'weight_decay': weight_decay})\n",
    "        return groups\n",
    "\n",
    "    def forward(self, video_feat, text_feat, caption_feat, video_mask=None, text_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the multimodal model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        video_feat : torch.Tensor\n",
    "            Video features.\n",
    "        text_feat : torch.Tensor\n",
    "            Text features.\n",
    "        caption_feat : torch.Tensor\n",
    "            Caption features.\n",
    "        video_mask : torch.Tensor, optional\n",
    "            Mask for video tokens.\n",
    "        text_mask : torch.Tensor, optional\n",
    "            Mask for text tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Class logits for each sample.\n",
    "        \"\"\"\n",
    "        ca1_out = self.ca1(video_feat, text_feat, video_mask=video_mask, text_mask=text_mask)\n",
    "        ca1_out = self.fc_ca1(ca1_out)\n",
    "        ca1_out = self.dropout(ca1_out)\n",
    "        return self.late_fusion(ca1_out, caption_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90cfb1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training SV-FEND-like ===\n",
      "Epoch 1/15 - Train Loss: 0.9079, Val Acc: 0.6099\n",
      "Epoch 2/15 - Train Loss: 0.7645, Val Acc: 0.6455\n",
      "Epoch 3/15 - Train Loss: 0.6889, Val Acc: 0.6495\n",
      "Epoch 4/15 - Train Loss: 0.6335, Val Acc: 0.6495\n",
      "Epoch 5/15 - Train Loss: 0.5877, Val Acc: 0.6752\n",
      "Epoch 6/15 - Train Loss: 0.5419, Val Acc: 0.6594\n",
      "Epoch 7/15 - Train Loss: 0.5053, Val Acc: 0.6851\n",
      "Epoch 8/15 - Train Loss: 0.4646, Val Acc: 0.6792\n",
      "Epoch 9/15 - Train Loss: 0.4353, Val Acc: 0.6653\n",
      "Epoch 10/15 - Train Loss: 0.3956, Val Acc: 0.6772\n",
      "Epoch 11/15 - Train Loss: 0.3611, Val Acc: 0.6693\n",
      "Epoch 12/15 - Train Loss: 0.3331, Val Acc: 0.6713\n",
      "Epoch 13/15 - Train Loss: 0.3052, Val Acc: 0.6733\n",
      "Epoch 14/15 - Train Loss: 0.2811, Val Acc: 0.6653\n",
      "Epoch 15/15 - Train Loss: 0.2514, Val Acc: 0.6594\n",
      "\n",
      "=== Training MiniMMBT ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jumita/miniconda3/envs/sd-webui/lib/python3.11/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Train Loss: 0.9438, Val Acc: 0.5762\n",
      "Epoch 2/15 - Train Loss: 0.7732, Val Acc: 0.5921\n",
      "Epoch 3/15 - Train Loss: 0.6606, Val Acc: 0.6376\n",
      "Epoch 4/15 - Train Loss: 0.5125, Val Acc: 0.6119\n",
      "Epoch 5/15 - Train Loss: 0.3367, Val Acc: 0.6396\n",
      "Epoch 6/15 - Train Loss: 0.2222, Val Acc: 0.5901\n",
      "Epoch 7/15 - Train Loss: 0.1435, Val Acc: 0.6119\n",
      "Epoch 8/15 - Train Loss: 0.0959, Val Acc: 0.6198\n",
      "Epoch 9/15 - Train Loss: 0.0652, Val Acc: 0.5960\n",
      "Epoch 10/15 - Train Loss: 0.0749, Val Acc: 0.6079\n",
      "Epoch 11/15 - Train Loss: 0.0646, Val Acc: 0.5802\n",
      "Epoch 12/15 - Train Loss: 0.0440, Val Acc: 0.5980\n",
      "Epoch 13/15 - Train Loss: 0.0567, Val Acc: 0.6040\n",
      "Epoch 14/15 - Train Loss: 0.0283, Val Acc: 0.5941\n",
      "Epoch 15/15 - Train Loss: 0.0441, Val Acc: 0.6040\n",
      "\n",
      "=== Training LateFusionMLP ===\n",
      "Epoch 1/15 - Train Loss: 0.9099, Val Acc: 0.6079\n",
      "Epoch 2/15 - Train Loss: 0.7658, Val Acc: 0.6317\n",
      "Epoch 3/15 - Train Loss: 0.6939, Val Acc: 0.6673\n",
      "Epoch 4/15 - Train Loss: 0.6382, Val Acc: 0.6495\n",
      "Epoch 5/15 - Train Loss: 0.5917, Val Acc: 0.6594\n",
      "Epoch 6/15 - Train Loss: 0.5504, Val Acc: 0.6772\n",
      "Epoch 7/15 - Train Loss: 0.5099, Val Acc: 0.6792\n",
      "Epoch 8/15 - Train Loss: 0.4695, Val Acc: 0.6891\n",
      "Epoch 9/15 - Train Loss: 0.4382, Val Acc: 0.6614\n",
      "Epoch 10/15 - Train Loss: 0.4018, Val Acc: 0.6733\n",
      "Epoch 11/15 - Train Loss: 0.3672, Val Acc: 0.6752\n",
      "Epoch 12/15 - Train Loss: 0.3390, Val Acc: 0.6693\n",
      "Epoch 13/15 - Train Loss: 0.3140, Val Acc: 0.6733\n",
      "Epoch 14/15 - Train Loss: 0.2803, Val Acc: 0.6752\n",
      "Epoch 15/15 - Train Loss: 0.2534, Val Acc: 0.6871\n",
      "\n",
      "=== Logistic Regression ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jumita/miniconda3/envs/sd-webui/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmark Results ===\n",
      "SV-FEND: Acc=0.6713, F1=0.6693, Prec=0.6727, Rec=0.6673\n",
      "MiniMMBT: Acc=0.5980, F1=0.5932, Prec=0.6021, Rec=0.5919\n",
      "LateFusionMLP: Acc=0.6772, F1=0.6768, Prec=0.6769, Rec=0.6768\n",
      "LogisticRegression: Acc=0.5802, F1=0.5782, Prec=0.5779, Rec=0.5789\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ================= Device =================\n",
    "device = \"mps\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ================= Label mapping =================\n",
    "unique_labels = [\"0\", \"1\", \"2\"]\n",
    "label2id = {name: i for i, name in enumerate(unique_labels)}\n",
    "\n",
    "# ================= Placeholder feature extractors =================\n",
    "def extract_video_feature(video_str):\n",
    "    \"\"\"\n",
    "    Simulate video feature extraction.\n",
    "\n",
    "    Args:\n",
    "        video_str (str): Placeholder video identifier.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Random 256-dimensional video feature vector.\n",
    "    \"\"\"\n",
    "    return torch.randn(256, dtype=torch.float32)\n",
    "\n",
    "def extract_text_embedding(text_str):\n",
    "    \"\"\"\n",
    "    Simulate text or caption feature extraction.\n",
    "\n",
    "    Args:\n",
    "        text_str (str): Placeholder text identifier.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Random 256-dimensional text feature vector.\n",
    "    \"\"\"\n",
    "    return torch.randn(256, dtype=torch.float32)\n",
    "\n",
    "train_dataset = VideoTextCaptionDataset(data_train, labels_train)\n",
    "val_dataset   = VideoTextCaptionDataset(data_val, labels_val)\n",
    "test_dataset  = VideoTextCaptionDataset(data_test, labels_test)\n",
    "\n",
    "\n",
    "# ================= Collate function =================\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate a batch of dataset items into tensors suitable for DataLoader.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of dataset items (video_feat, text_feat, caption_feat, label).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Batched tensors (video_feats, text_feats, caption_feats, labels)\n",
    "    \"\"\"\n",
    "    video_feats = torch.stack([item[\"video_feat\"] for item in batch])\n",
    "    text_feats = torch.stack([item[\"text_feat\"] for item in batch])\n",
    "    caption_feats = torch.stack([item[\"caption_feat\"] for item in batch])\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
    "    return video_feats, text_feats, caption_feats, labels\n",
    "\n",
    "# ================= Baselines =================\n",
    "class SVFENDBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP baseline combining video, text, and caption features.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, num_classes=3):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            video (torch.Tensor): Video features.\n",
    "            text (torch.Tensor): Text features.\n",
    "            caption (torch.Tensor): Caption features.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Class logits.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(feature_dim*3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, video, text, caption):\n",
    "        x = torch.cat([video, text, caption], dim=1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class MiniMMBT(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini Multimodal Bitransformer (MiniMMBT) baseline.\n",
    "    Combines video and text sequences via a Transformer encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, num_classes=3, num_layers=2):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=8)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(feature_dim, num_classes)\n",
    "    def forward(self, video, text):\n",
    "        \"\"\"\n",
    "        Forward pass for MiniMMBT.\n",
    "\n",
    "        Args:\n",
    "            video (torch.Tensor): Video features of shape (batch, seq_len, feature_dim) or (batch, feature_dim)\n",
    "            text (torch.Tensor): Text features of shape (batch, seq_len, feature_dim) or (batch, feature_dim)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Class logits.\n",
    "        \"\"\"\n",
    "        if video.dim() == 2: video = video.unsqueeze(1)\n",
    "        if text.dim() == 2: text = text.unsqueeze(1)\n",
    "        x = torch.cat([video, text], dim=1)\n",
    "        x = self.transformer(x)\n",
    "        cls_token = x[:,0,:]\n",
    "        return self.fc(cls_token)\n",
    "\n",
    "class LateFusionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Late fusion baseline that averages temporal features before MLP classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(feature_dim*3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, video, text, caption):\n",
    "        \"\"\"\n",
    "        Forward pass for late fusion.\n",
    "\n",
    "        Args:\n",
    "            video (torch.Tensor): Video features (batch, seq_len, feature_dim) or (batch, feature_dim)\n",
    "            text (torch.Tensor): Text features (batch, seq_len, feature_dim) or (batch, feature_dim)\n",
    "            caption (torch.Tensor): Caption features (batch, feature_dim)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Class logits.\n",
    "        \"\"\"\n",
    "        if video.dim() == 3: video = video.mean(dim=1)\n",
    "        if text.dim() == 3: text = text.mean(dim=1)\n",
    "        x = torch.cat([video, text, caption], dim=1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "# ================= Training / Testing =================\n",
    "def train_model(model, train_loader, val_loader, num_epochs=15, lr=2e-4):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to train.\n",
    "        train_loader (DataLoader): Training DataLoader.\n",
    "        val_loader (DataLoader): Validation DataLoader.\n",
    "        num_epochs (int): Number of epochs.\n",
    "        lr (float): Learning rate.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Trained model.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for video, text, caption, label in train_loader:\n",
    "            video, text, caption, label = video.to(device), text.to(device), caption.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(model, MiniMMBT):\n",
    "                output = model(video, text)\n",
    "            else:\n",
    "                output = model(video, text, caption)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for video, text, caption, label in val_loader:\n",
    "                video, text, caption, label = video.to(device), text.to(device), caption.to(device), label.to(device)\n",
    "                if isinstance(model, MiniMMBT):\n",
    "                    output = model(video, text)\n",
    "                else:\n",
    "                    output = model(video, text, caption)\n",
    "                preds = output.argmax(dim=-1)\n",
    "                correct += (preds == label).sum().item()\n",
    "                total += label.size(0)\n",
    "        val_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Test a trained PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        test_loader (DataLoader): Test DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (accuracy, f1, precision, recall)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for video, text, caption, label in test_loader:\n",
    "            video, text, caption, label = video.to(device), text.to(device), caption.to(device), label.to(device)\n",
    "            if isinstance(model, MiniMMBT):\n",
    "                output = model(video, text)\n",
    "            else:\n",
    "                output = model(video, text, caption)\n",
    "            preds = output.argmax(dim=-1)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(label.cpu())\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    prec = precision_score(all_labels, all_preds, average='macro')\n",
    "    rec = recall_score(all_labels, all_preds, average='macro')\n",
    "    return acc, f1, prec, rec\n",
    "\n",
    "def train_logistic_regression(train_loader, val_loader):\n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier on extracted features.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        val_loader (DataLoader): Validation data loader.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (accuracy, f1, precision, recall) on validation set.\n",
    "    \"\"\"\n",
    "    X_train, y_train = [], []\n",
    "    X_val, y_val = [], []\n",
    "    for video, text, caption, label in train_loader:\n",
    "        X_train.append(torch.cat([video, text, caption], dim=1).cpu().numpy())\n",
    "        y_train.append(label.cpu().numpy())\n",
    "    for video, text, caption, label in val_loader:\n",
    "        X_val.append(torch.cat([video, text, caption], dim=1).cpu().numpy())\n",
    "        y_val.append(label.cpu().numpy())\n",
    "\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    X_val = np.concatenate(X_val, axis=0)\n",
    "    y_val = np.concatenate(y_val, axis=0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    prec = precision_score(y_val, y_pred, average='macro')\n",
    "    rec = recall_score(y_val, y_pred, average='macro')\n",
    "    return acc, f1, prec, rec\n",
    "\n",
    "# ================= Run all baselines =================\n",
    "def run_all_baselines(train_loader, val_loader, test_loader, feature_dim=256, num_classes=3):\n",
    "    \"\"\"\n",
    "    Run all baseline models: SV-FEND, MiniMMBT, LateFusionMLP, and Logistic Regression.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        val_loader (DataLoader): Validation data loader.\n",
    "        test_loader (DataLoader): Test data loader.\n",
    "        feature_dim (int): Feature dimensionality.\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of results for each baseline with metrics (acc, f1, precision, recall).\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    print(\"=== Training SV-FEND-like ===\")\n",
    "    svfend = SVFENDBaseline(feature_dim, num_classes)\n",
    "    svfend = train_model(svfend, train_loader, val_loader)\n",
    "    results['SV-FEND'] = test_model(svfend, test_loader)\n",
    "\n",
    "    print(\"\\n=== Training MiniMMBT ===\")\n",
    "    mmbt = MiniMMBT(feature_dim, num_classes)\n",
    "    mmbt = train_model(mmbt, train_loader, val_loader)\n",
    "    results['MiniMMBT'] = test_model(mmbt, test_loader)\n",
    "\n",
    "    print(\"\\n=== Training LateFusionMLP ===\")\n",
    "    lfm = LateFusionMLP(feature_dim, num_classes)\n",
    "    lfm = train_model(lfm, train_loader, val_loader)\n",
    "    results['LateFusionMLP'] = test_model(lfm, test_loader)\n",
    "\n",
    "    print(\"\\n=== Logistic Regression ===\")\n",
    "    results['LogisticRegression'] = train_logistic_regression(train_loader, val_loader)\n",
    "\n",
    "    print(\"\\n=== Benchmark Results ===\")\n",
    "    for k, v in results.items():\n",
    "        acc, f1, prec, rec = v\n",
    "        print(f\"{k}: Acc={acc:.4f}, F1={f1:.4f}, Prec={prec:.4f}, Rec={rec:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ================= USAGE EXAMPLE =================\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "results = run_all_baselines(train_loader, val_loader, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd-webui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
