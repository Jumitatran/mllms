{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1094bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON loaded, items: 5047\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_or_create_data(json_path, excel_path, labels_path, video_dir, text_dir, caption_dir):\n",
    "    \"\"\"\n",
    "    Load labels from NPY and JSON. If JSON is missing or mismatched, generate it from Excel.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): path to the JSON file storing data_list\n",
    "        excel_path (str): path to the Excel file containing the video list\n",
    "        labels_path (str): path to the NPY file containing labels\n",
    "        video_dir (str): directory containing .npy video features\n",
    "        text_dir (str): directory containing .npy text features\n",
    "        caption_dir (str): directory containing .npy caption features\n",
    "\n",
    "    Returns:\n",
    "        data_list (list of dict): list of dictionaries with keys 'video_feat', 'text_feat', 'caption_feat'\n",
    "        labels (np.ndarray): labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Load labels\n",
    "    if not os.path.exists(labels_path):\n",
    "        raise FileNotFoundError(f\"Labels file not found: {labels_path}\")\n",
    "    labels = np.load(labels_path)\n",
    "\n",
    "    # Check JSON\n",
    "    need_create_json = True\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data_list = json.load(f)\n",
    "                if len(data_list) == len(labels):\n",
    "                    need_create_json = False\n",
    "            except json.JSONDecodeError:\n",
    "                need_create_json = True\n",
    "\n",
    "    # Create JSON \n",
    "    if need_create_json:\n",
    "        if not os.path.exists(excel_path):\n",
    "            raise FileNotFoundError(f\"Excel file not found: {excel_path}\")\n",
    "        df = pd.read_excel(excel_path)\n",
    "        data_list = []\n",
    "        for vid in df['video_id'].astype(str):\n",
    "            data_list.append({\n",
    "                \"name\": vid,\n",
    "                \"video_feat\": os.path.join(video_dir, f\"{vid}.npy\"),\n",
    "                \"text_feat\": os.path.join(text_dir, f\"{vid}.npy\"),\n",
    "                \"caption_feat\": os.path.join(caption_dir, f\"{vid}.npy\")\n",
    "            })\n",
    "        # Create folder\n",
    "        os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "        # Save JSON\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data_list, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"JSON created, items: {len(data_list)}\")\n",
    "    else:\n",
    "        print(f\"JSON loaded, items: {len(data_list)}\")\n",
    "\n",
    "    return data_list, labels\n",
    "\n",
    "video_dir = \"/Users/jumita/Downloads/final/frame\"\n",
    "text_dir = \"/Users/jumita/Downloads/final/text\"\n",
    "caption_dir = \"/Users/jumita/Downloads/final/caption\"\n",
    "json_path = \"/Users/jumita/Downloads/final_code/data.json\"\n",
    "excel_path = \"/Users/jumita/Downloads/Book5.xlsx\"\n",
    "labels_path = \"/Users/jumita/Downloads/final_code/labels.npy\"\n",
    "\n",
    "data_list, labels = load_or_create_data(\n",
    "    json_path=json_path,\n",
    "    excel_path=excel_path,\n",
    "    labels_path=labels_path,\n",
    "    video_dir=video_dir,\n",
    "    text_dir=text_dir,\n",
    "    caption_dir=caption_dir\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f770ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON loaded, items: 5047\n",
      "Overlap train-val: 0\n",
      "Overlap train-test: 0\n",
      "Overlap val-test: 0\n",
      "Augmented sample shapes:\n",
      "Video: torch.Size([16, 256])\n",
      "Text: torch.Size([16, 256])\n",
      "Caption: torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ================= VIDEO FEATURE AUGMENTER =================\n",
    "class VideoFeatureAugmenter:\n",
    "    \"\"\"\n",
    "    Data augmenter for video features.\n",
    "\n",
    "    This class applies temporal cropping and adaptive Gaussian noise \n",
    "    to video features to improve model generalization during training.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    augment_prob : float\n",
    "        Probability of applying augmentation.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer used for augmentation (optional, not used here).\n",
    "    noise_scale : float\n",
    "        Scaling factor for Gaussian noise.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    temporal_crop(features, crop_ratio=0.9):\n",
    "        Randomly crops the sequence along the temporal dimension.\n",
    "    adaptive_noise(features):\n",
    "        Adds Gaussian noise proportional to the feature's standard deviation.\n",
    "    __call__(features):\n",
    "        Applies augmentation with probability `augment_prob`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, augment_prob=0.5, dropout_prob=0.3, noise_scale=0.05):\n",
    "        self.augment_prob = augment_prob\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "    def temporal_crop(self, features, crop_ratio=0.9):\n",
    "        \"\"\"\n",
    "        Crop the feature sequence randomly along the temporal dimension.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Video feature tensor of shape (T, D) or (T, H, D).\n",
    "        crop_ratio : float\n",
    "            Fraction of the sequence to keep.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Cropped feature tensor.\n",
    "        \"\"\"\n",
    "        T = features.shape[0]\n",
    "        new_T = int(T * crop_ratio)\n",
    "        start = np.random.randint(0, T - new_T + 1)\n",
    "        return features[start:start+new_T]\n",
    "\n",
    "    def adaptive_noise(self, features):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise scaled by feature standard deviation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Noisy feature tensor.\n",
    "        \"\"\"\n",
    "        std_per_feature = features.std(dim=0, keepdim=True) + 1e-6\n",
    "        noise = torch.randn_like(features) * std_per_feature * self.noise_scale\n",
    "        return features + noise\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Apply augmentation with probability `augment_prob`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Input feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Augmented feature tensor.\n",
    "        \"\"\"\n",
    "        if torch.rand(1) < self.augment_prob:\n",
    "            if features.ndim >= 2:\n",
    "                features = self.temporal_crop(features)\n",
    "            features = self.adaptive_noise(features)\n",
    "        return features\n",
    "\n",
    "# ================= TEXT/CAPTION FEATURE AUGMENTER =================\n",
    "class TextFeatureAugmenter:\n",
    "    \"\"\"\n",
    "    Data augmenter for text or caption features.\n",
    "\n",
    "    Applies dropout and adaptive Gaussian noise to embeddings.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    augment_prob : float\n",
    "        Probability of applying augmentation.\n",
    "    dropout : nn.Dropout\n",
    "        Dropout layer applied to features.\n",
    "    noise_scale : float\n",
    "        Scaling factor for Gaussian noise.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    adaptive_noise(features):\n",
    "        Adds Gaussian noise proportional to feature's std.\n",
    "    __call__(features):\n",
    "        Applies augmentation with probability `augment_prob`.\n",
    "    \"\"\"\n",
    "    def __init__(self, augment_prob=0.3, dropout_prob=0.05, noise_scale=0.02):\n",
    "        self.augment_prob = augment_prob\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "    def adaptive_noise(self, features):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise scaled by feature standard deviation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Input feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Noisy feature tensor.\n",
    "        \"\"\"\n",
    "        std_per_feature = features.std(dim=0, keepdim=True) + 1e-6\n",
    "        noise = torch.randn_like(features) * std_per_feature * self.noise_scale\n",
    "        return features + noise\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Apply dropout and noise augmentation with probability `augment_prob`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : torch.Tensor\n",
    "            Input feature tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Augmented feature tensor.\n",
    "        \"\"\"\n",
    "        if torch.rand(1) < self.augment_prob:\n",
    "            features = self.dropout(features)\n",
    "            features = self.adaptive_noise(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "# ================= DATASET =================\n",
    "class VideoTextCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for multimodal video, text, and caption features.\n",
    "\n",
    "    Handles optional data augmentation and per-sample feature normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_list : list of dict\n",
    "        List of dictionaries containing paths for 'video_feat', 'text_feat', 'caption_feat'.\n",
    "    labels : np.ndarray\n",
    "        Array of integer labels corresponding to each sample.\n",
    "    augment : bool, optional\n",
    "        Whether to apply data augmentation (default: False).\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(idx):\n",
    "        Loads, optionally augments, and returns a single sample.\n",
    "    __len__():\n",
    "        Returns the total number of samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, labels, augment=False):\n",
    "        self.data_list = data_list\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        self.video_aug = VideoFeatureAugmenter() if augment else None\n",
    "        self.text_aug = TextFeatureAugmenter() if augment else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and process a single sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary with keys: 'video_feat', 'text_feat', 'caption_feat', 'label'.\n",
    "        \"\"\"\n",
    "        item = self.data_list[idx]\n",
    "\n",
    "        video_feat = torch.from_numpy(np.load(item[\"video_feat\"])).float()\n",
    "        text_feat = torch.from_numpy(np.load(item[\"text_feat\"])).float()\n",
    "        caption_feat = torch.from_numpy(np.load(item[\"caption_feat\"])).float()\n",
    "\n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment:\n",
    "            if self.video_aug and video_feat.ndim >= 2:  \n",
    "                video_feat = self.video_aug(video_feat)\n",
    "            if self.text_aug:\n",
    "                text_feat = self.text_aug(text_feat)\n",
    "                caption_feat = self.text_aug(caption_feat)\n",
    "\n",
    "        # Flatten features if needed\n",
    "        if video_feat.ndim == 3:\n",
    "            video_feat = video_feat.mean(dim=(0,1))\n",
    "        elif video_feat.ndim == 2:\n",
    "            video_feat = video_feat.mean(dim=0)\n",
    "        \n",
    "        text_feat = text_feat if text_feat.ndim == 1 else text_feat.mean(dim=0)\n",
    "        caption_feat = caption_feat if caption_feat.ndim == 1 else caption_feat.mean(dim=0)\n",
    "\n",
    "        # Normalize each feature\n",
    "        video_feat = (video_feat - video_feat.mean()) / (video_feat.std() + 1e-6)\n",
    "        text_feat = (text_feat - text_feat.mean()) / (text_feat.std() + 1e-6)\n",
    "        caption_feat = (caption_feat - caption_feat.mean()) / (caption_feat.std() + 1e-6)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"video_feat\": video_feat,\n",
    "            \"text_feat\": text_feat,\n",
    "            \"caption_feat\": caption_feat,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "def create_dataloader(data_list, labels, batch_size=16, shuffle=True, augment=False):\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader for the multimodal dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_list : list of dict\n",
    "        List of dictionaries containing feature file paths.\n",
    "    labels : np.ndarray\n",
    "        Array of labels.\n",
    "    batch_size : int, optional\n",
    "        Batch size (default: 16).\n",
    "    shuffle : bool, optional\n",
    "        Whether to shuffle data (default: True).\n",
    "    augment : bool, optional\n",
    "        Whether to apply data augmentation (default: False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataLoader\n",
    "        PyTorch DataLoader for the dataset.\n",
    "    \"\"\"\n",
    "    dataset = VideoTextCaptionDataset(data_list, labels, augment=augment)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# ================= LOAD DATA =================\n",
    "data_list, labels = load_or_create_data(\n",
    "    json_path=\"/Users/jumita/Downloads/final_code/data.json\",\n",
    "    excel_path=\"/Users/jumita/Downloads/Book5.xlsx\",\n",
    "    labels_path=\"/Users/jumita/Downloads/final_code/labels.npy\",\n",
    "    video_dir=\"/Users/jumita/Downloads/final/frame\",\n",
    "    text_dir=\"/Users/jumita/Downloads/final/text\",\n",
    "    caption_dir=\"/Users/jumita/Downloads/final/caption\"\n",
    ")\n",
    "\n",
    "# ================= SPLIT DATA =================\n",
    "data_train, data_temp, labels_train, labels_temp = train_test_split(\n",
    "    data_list, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "try:\n",
    "    data_val, data_test, labels_val, labels_test = train_test_split(\n",
    "        data_temp, labels_temp, test_size=0.5, random_state=42, stratify=labels_temp\n",
    "    )\n",
    "except ValueError:\n",
    "    data_val, data_test, labels_val, labels_test = train_test_split(\n",
    "        data_temp, labels_temp, test_size=0.5, random_state=42, stratify=None\n",
    "    )\n",
    "\n",
    "def check_overlap(set1, set2):\n",
    "    s1 = set([item[\"video_feat\"] for item in set1])\n",
    "    s2 = set([item[\"video_feat\"] for item in set2])\n",
    "    return len(s1 & s2)\n",
    "\n",
    "print(\"Overlap train-val:\", check_overlap(data_train, data_val))\n",
    "print(\"Overlap train-test:\", check_overlap(data_train, data_test))\n",
    "print(\"Overlap val-test:\", check_overlap(data_val, data_test))\n",
    "\n",
    "\n",
    "# ================= DATALOADERS =================\n",
    "train_loader = create_dataloader(data_train, labels_train, batch_size=16, shuffle=True, augment=True)\n",
    "val_loader = create_dataloader(data_val, labels_val, batch_size=16, shuffle=False)\n",
    "test_loader = create_dataloader(data_test, labels_test, batch_size=16, shuffle=False)\n",
    "\n",
    "# ================= DEBUG SAMPLE =================\n",
    "sample = next(iter(train_loader))\n",
    "print(\"Augmented sample shapes:\")\n",
    "print(\"Video:\", sample[\"video_feat\"].shape)\n",
    "print(\"Text:\", sample[\"text_feat\"].shape)\n",
    "print(\"Caption:\", sample[\"caption_feat\"].shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300b4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ================= StochasticDepth =================\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Stochastic Depth (also called DropPath) regularization.\n",
    "\n",
    "    During training, randomly drops entire residual paths with probability `drop_prob`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    drop_prob : float\n",
    "        Probability of dropping a path.\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with stochastic depth.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, ...).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor, either dropped or scaled.\n",
    "        \"\"\"\n",
    "        if not self.training or self.drop_prob == 0.0:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        mask = (torch.rand(x.shape[0], device=x.device) < keep_prob).to(x.dtype)\n",
    "        mask = mask.view(-1, *([1] * (x.dim() - 1)))\n",
    "        return x * mask / keep_prob\n",
    "\n",
    "# ================= LoRA Linear =================\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with Low-Rank Adaptation (LoRA).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Input feature dimension.\n",
    "    out_features : int\n",
    "        Output feature dimension.\n",
    "    r : int\n",
    "        Rank of the LoRA adaptation matrices.\n",
    "    alpha : int\n",
    "        Scaling factor for LoRA updates.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, r=6, alpha=6):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / max(1, r)\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(torch.randn(r, in_features) * 1e-3)\n",
    "            self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through LoRA linear layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, in_features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (B, out_features).\n",
    "        \"\"\"\n",
    "        result = self.linear(x)\n",
    "        if self.r > 0:\n",
    "            lora_update = (x @ self.lora_A.T) @ self.lora_B.T\n",
    "            result += lora_update * self.scaling\n",
    "        return result\n",
    "\n",
    "# ================= LoRA MultiheadAttention =================\n",
    "class LoRAMultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention layer with LoRA adaptation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embed_dim : int\n",
    "        Dimension of embeddings.\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    dropout : float\n",
    "        Dropout probability in attention.\n",
    "    batch_first : bool\n",
    "        If True, input shape is (B, S, D).\n",
    "    r : int\n",
    "        LoRA rank.\n",
    "    alpha : int\n",
    "        LoRA scaling factor.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0, batch_first=True, r=6, alpha=6):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / max(1, r)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=batch_first)\n",
    "        if r > 0:\n",
    "            self.lora_A_q = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_q = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "            self.lora_A_k = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_k = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "            self.lora_A_v = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_v = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "            self.lora_A_out = nn.Parameter(torch.randn(r, embed_dim) * 1e-3)\n",
    "            self.lora_B_out = nn.Parameter(torch.zeros(embed_dim, r))\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=False, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through LoRA multihead attention.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query, key, value : torch.Tensor\n",
    "            Input tensors of shape (B, S, D).\n",
    "        key_padding_mask : torch.Tensor, optional\n",
    "            Mask for padded tokens.\n",
    "        need_weights : bool\n",
    "            If True, returns attention weights.\n",
    "        attn_mask : torch.Tensor, optional\n",
    "            Attention mask.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        attn_output : torch.Tensor\n",
    "            Attention output tensor.\n",
    "        attn_weights : torch.Tensor\n",
    "            Attention weights (if `need_weights` is True).\n",
    "        \"\"\"\n",
    "        if self.r > 0:\n",
    "            query = query + (query @ self.lora_A_q.T) @ self.lora_B_q.T * self.scaling\n",
    "            key   = key   + (key   @ self.lora_A_k.T) @ self.lora_B_k.T * self.scaling\n",
    "            value = value + (value @ self.lora_A_v.T) @ self.lora_B_v.T * self.scaling\n",
    "\n",
    "        attn_output, attn_weights = self.attn(\n",
    "            query, key, value,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=need_weights,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "\n",
    "        if self.r > 0:\n",
    "            attn_output = attn_output + (attn_output @ self.lora_A_out.T) @ self.lora_B_out.T * self.scaling\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "# ================= CoAttentionBlock =================\n",
    "class CoAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Co-Attention block between query and key/value features with LoRA and residual feed-forward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_q : int\n",
    "        Dimension of query features.\n",
    "    dim_kv : int\n",
    "        Dimension of key/value features.\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    hidden_dim : int\n",
    "        Hidden dimension for feed-forward network.\n",
    "    dropout : float\n",
    "        Dropout probability.\n",
    "    lora_r : int\n",
    "        LoRA rank.\n",
    "    drop_path_prob : float\n",
    "        Stochastic depth probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_q=256, dim_kv=256, num_heads=8, hidden_dim=256, dropout=0.2, lora_r=6, drop_path_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.query_proj = LoRALinear(dim_q, dim_q, r=lora_r)\n",
    "        self.key_proj   = LoRALinear(dim_kv, dim_q, r=lora_r)\n",
    "        self.value_proj = LoRALinear(dim_kv, dim_q, r=lora_r)\n",
    "        self.attn = LoRAMultiheadAttention(dim_q, num_heads, dropout=dropout, batch_first=True, r=lora_r)\n",
    "        self.gate = nn.Sequential(LoRALinear(dim_q * 2, dim_q, r=lora_r), nn.Sigmoid())\n",
    "        self.ffn = nn.Sequential(\n",
    "            LoRALinear(dim_q, hidden_dim, r=lora_r),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            LoRALinear(hidden_dim, dim_q, r=lora_r),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim_q)\n",
    "        self.norm2 = nn.LayerNorm(dim_q)\n",
    "        self.drop_path = StochasticDepth(drop_path_prob) if drop_path_prob > 0.0 else None\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of CoAttentionBlock.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query, key, value : torch.Tensor\n",
    "            Input feature tensors.\n",
    "        key_padding_mask : torch.Tensor, optional\n",
    "            Mask for padded tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor after co-attention, gating, and feed-forward.\n",
    "        \"\"\"\n",
    "        q = self.query_proj(query)\n",
    "        k = self.key_proj(key)\n",
    "        v = self.value_proj(value)\n",
    "        attn_out, _ = self.attn(q, k, v, key_padding_mask=key_padding_mask)\n",
    "        gate = self.gate(torch.cat([query, attn_out], dim=-1))\n",
    "        out = self.norm1(query + gate * attn_out)\n",
    "        ffn_out = self.ffn(out)\n",
    "        if self.drop_path is not None:\n",
    "            ffn_out = self.drop_path(ffn_out)\n",
    "        return self.norm2(out + ffn_out)\n",
    "\n",
    "# ================= CoAttentionLayer =================\n",
    "class CoAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Co-Attention layer for interaction between video and text features.\n",
    "\n",
    "    It applies co-attention blocks in both directions:\n",
    "    video-to-text and text-to-video, then pools the sequences\n",
    "    and concatenates the outputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_q : int\n",
    "        Dimension of query features (video features).\n",
    "    dim_kv : int\n",
    "        Dimension of key/value features (text features).\n",
    "    num_heads : int\n",
    "        Number of attention heads.\n",
    "    hidden_dim : int\n",
    "        Hidden dimension for feed-forward networks.\n",
    "    dropout : float\n",
    "        Dropout probability.\n",
    "    lora_r : int\n",
    "        LoRA rank.\n",
    "    max_video_tokens : int\n",
    "        Maximum number of video tokens to keep after pooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_q=256, dim_kv=256, num_heads=8, hidden_dim=256, dropout=0.5, lora_r=6, max_video_tokens=32):\n",
    "        super().__init__()\n",
    "        self.video2text = CoAttentionBlock(dim_q, dim_kv, num_heads, hidden_dim, dropout, lora_r)\n",
    "        self.text2video = CoAttentionBlock(dim_kv, dim_q, num_heads, hidden_dim, dropout, lora_r)\n",
    "        self.max_video_tokens = max_video_tokens\n",
    "\n",
    "    def forward(self, video_feat, text_feat, video_mask=None, text_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of co-attention layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        video_feat : torch.Tensor\n",
    "            Video features, shape (B, T, D) or (B, N, T, D) if patches.\n",
    "        text_feat : torch.Tensor\n",
    "            Text features, shape (B, S, D).\n",
    "        video_mask : torch.Tensor, optional\n",
    "            Mask for video tokens.\n",
    "        text_mask : torch.Tensor, optional\n",
    "            Mask for text tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Concatenated pooled co-attention features, shape (B, D_video + D_text).\n",
    "        \"\"\"\n",
    "        B = video_feat.shape[0]\n",
    "\n",
    "        # Flatten video patches if needed\n",
    "        if video_feat.dim() == 4:  \n",
    "            T, N, D = video_feat.shape[1], video_feat.shape[2], video_feat.shape[3]\n",
    "            video_feat = video_feat.view(B, T * N, D)\n",
    "        elif video_feat.dim() == 2:\n",
    "            video_feat = video_feat.unsqueeze(1)\n",
    "\n",
    "        # Pooling video sequence\n",
    "        if video_feat.shape[1] > self.max_video_tokens:\n",
    "            video_feat = video_feat.transpose(1, 2)  # (B, D, Seq)\n",
    "            video_feat = F.adaptive_avg_pool1d(video_feat, self.max_video_tokens)\n",
    "            video_feat = video_feat.transpose(1, 2)\n",
    "\n",
    "        if text_feat.dim() == 2:\n",
    "            text_feat = text_feat.unsqueeze(1)\n",
    "\n",
    "        # Forward co-attention with optional masks\n",
    "        v2t = self.video2text(video_feat, text_feat, text_feat, key_padding_mask=text_mask)\n",
    "        t2v = self.text2video(text_feat, video_feat, video_feat, key_padding_mask=video_mask)\n",
    "        \n",
    "        # Pool sequences\n",
    "        v2t_pooled = v2t.mean(dim=1)\n",
    "        t2v_pooled = t2v.mean(dim=1)\n",
    "        return torch.cat([v2t_pooled, t2v_pooled], dim=-1)\n",
    "\n",
    "# ================= LateFusionLoRA =================\n",
    "class LateFusionLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Late fusion module with LoRA for combining two feature modalities.\n",
    "\n",
    "    Applies two-layer MLP with LayerNorm, GELU activation, dropout,\n",
    "    and optional stochastic depth, then outputs class logits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_feat1 : int\n",
    "        Dimension of first input feature.\n",
    "    dim_feat2 : int\n",
    "        Dimension of second input feature.\n",
    "    dim_hidden : int\n",
    "        Hidden layer dimension.\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "    r : int\n",
    "        LoRA rank.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_feat1, dim_feat2, dim_hidden, num_classes, r=6):\n",
    "        super().__init__()\n",
    "        self.fc1 = LoRALinear(dim_feat1 + dim_feat2, dim_hidden * 2, r=r)\n",
    "        self.norm1 = nn.LayerNorm(dim_hidden * 2)\n",
    "        self.fc2 = LoRALinear(dim_hidden * 2, dim_hidden, r=r)\n",
    "        self.norm2 = nn.LayerNorm(dim_hidden)\n",
    "        self.fc3 = LoRALinear(dim_hidden, num_classes, r=r)\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        self.drop_path = StochasticDepth(0.2) if dim_hidden > 256 else None\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Forward pass of late fusion module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x1, x2 : torch.Tensor\n",
    "            Input feature tensors to fuse, shape (B, dim_feat1), (B, dim_feat2).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output class logits, shape (B, num_classes).\n",
    "        \"\"\"\n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        if self.drop_path is not None:\n",
    "            x = self.drop_path(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# ================= MultimodalModel =================\n",
    "class MultimodalModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal model combining co-attention between video and text features\n",
    "    and late fusion with caption features, with LoRA finetuning support.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    video_dim : int\n",
    "        Dimension of video features.\n",
    "    text_dim : int\n",
    "        Dimension of text features.\n",
    "    caption_dim : int\n",
    "        Dimension of caption features.\n",
    "    hidden_dim : int\n",
    "        Hidden dimension for co-attention and fusion layers.\n",
    "    num_classes : int\n",
    "        Number of output classes.\n",
    "    r : int\n",
    "        LoRA rank.\n",
    "    finetune_mode : str\n",
    "        Finetuning mode: 'coattn_plus_latefusion', 'late_fusion_only', 'coattn_only', etc.\n",
    "    dropout_prob : float\n",
    "        Dropout probability for the fusion layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, video_dim=256, text_dim=256, caption_dim=256, hidden_dim=256,\n",
    "                 num_classes=3, r=6, finetune_mode: str = 'coattn_plus_latefusion',\n",
    "                 dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.ca1 = CoAttentionLayer(video_dim, text_dim, num_heads=8, hidden_dim=hidden_dim, lora_r=r)\n",
    "        self.fc_ca1 = LoRALinear(video_dim * 2, video_dim, r=r)\n",
    "        self.dropout = nn.Dropout(dropout_prob)   \n",
    "        self.late_fusion = LateFusionLoRA(video_dim, caption_dim, hidden_dim, num_classes, r=r)\n",
    "        self.set_trainable(finetune_mode)\n",
    "\n",
    "    def set_trainable(self, finetune_mode: str = 'coattn_plus_latefusion'):\n",
    "        \"\"\"\n",
    "        Set which parameters are trainable according to finetune_mode.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        finetune_mode : str\n",
    "            Mode of finetuning.\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        finetune_mode = finetune_mode.lower()\n",
    "\n",
    "        def enable_lora(module):\n",
    "            for name, p in module.named_parameters():\n",
    "                if \"lora_A\" in name or \"lora_B\" in name:\n",
    "                    p.requires_grad = True\n",
    "\n",
    "        def unfreeze_part(module, names_to_unfreeze):\n",
    "            for name, p in module.named_parameters():\n",
    "                for target_name in names_to_unfreeze:\n",
    "                    if target_name in name:\n",
    "                        p.requires_grad = True\n",
    "\n",
    "        if finetune_mode == 'late_fusion_only':\n",
    "            enable_lora(self.late_fusion)\n",
    "            enable_lora(self.fc_ca1)\n",
    "        elif finetune_mode == 'coattn_only':\n",
    "            enable_lora(self.ca1)\n",
    "            enable_lora(self.fc_ca1)\n",
    "        elif finetune_mode == 'coattn_plus_latefusion':\n",
    "            enable_lora(self.ca1)\n",
    "            enable_lora(self.late_fusion)\n",
    "            enable_lora(self.fc_ca1)\n",
    "        elif finetune_mode == 'coattn_plus_latefusion_part':\n",
    "            enable_lora(self.ca1)\n",
    "            enable_lora(self.late_fusion)\n",
    "            enable_lora(self.fc_ca1)\n",
    "            unfreeze_part(self.ca1.video2text.query_proj, ['linear'])\n",
    "            unfreeze_part(self.late_fusion.fc1, ['linear'])\n",
    "        elif finetune_mode == 'all':\n",
    "            for p in self.parameters():\n",
    "                p.requires_grad = True\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown finetune_mode: {finetune_mode}\")\n",
    "\n",
    "    def get_param_groups(self, lr_late=2e-4, lr_coattn=1.5e-4, lr_fc_ca1=5e-5, weight_decay=5e-4):\n",
    "        \"\"\"\n",
    "        Return parameter groups with separate learning rates for optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr_late, lr_coattn, lr_fc_ca1 : float\n",
    "            Learning rates for respective modules.\n",
    "        weight_decay : float\n",
    "            Weight decay for optimizer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of dict\n",
    "            Parameter groups for optimizer.\n",
    "        \"\"\"\n",
    "        groups = []\n",
    "        if any(p.requires_grad for p in self.late_fusion.parameters()):\n",
    "            groups.append({'params': [p for p in self.late_fusion.parameters() if p.requires_grad], 'lr': lr_late, 'weight_decay': weight_decay})\n",
    "        if any(p.requires_grad for p in self.ca1.parameters()):\n",
    "            groups.append({'params': [p for p in self.ca1.parameters() if p.requires_grad], 'lr': lr_coattn, 'weight_decay': weight_decay})\n",
    "        if any(p.requires_grad for p in self.fc_ca1.parameters()):\n",
    "            groups.append({'params': [p for p in self.fc_ca1.parameters() if p.requires_grad], 'lr': lr_fc_ca1, 'weight_decay': weight_decay})\n",
    "        return groups\n",
    "\n",
    "    def forward(self, video_feat, text_feat, caption_feat, video_mask=None, text_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the multimodal model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        video_feat : torch.Tensor\n",
    "            Video features.\n",
    "        text_feat : torch.Tensor\n",
    "            Text features.\n",
    "        caption_feat : torch.Tensor\n",
    "            Caption features.\n",
    "        video_mask : torch.Tensor, optional\n",
    "            Mask for video tokens.\n",
    "        text_mask : torch.Tensor, optional\n",
    "            Mask for text tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Class logits for each sample.\n",
    "        \"\"\"\n",
    "        ca1_out = self.ca1(video_feat, text_feat, video_mask=video_mask, text_mask=text_mask)\n",
    "        ca1_out = self.fc_ca1(ca1_out)\n",
    "        ca1_out = self.dropout(ca1_out)\n",
    "        return self.late_fusion(ca1_out, caption_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1afe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/3 ===\n",
      "Epoch 1/25 | Train Loss: 0.5100, Acc: 0.5033 | Val Loss: 0.9212, Acc: 0.6007\n",
      "Epoch 2/25 | Train Loss: 0.4574, Acc: 0.6070 | Val Loss: 0.8808, Acc: 0.6263\n",
      "Epoch 3/25 | Train Loss: 0.4320, Acc: 0.6391 | Val Loss: 0.8575, Acc: 0.6506\n",
      "Epoch 4/25 | Train Loss: 0.4110, Acc: 0.6647 | Val Loss: 0.8393, Acc: 0.6625\n",
      "Epoch 5/25 | Train Loss: 0.3963, Acc: 0.6920 | Val Loss: 0.8402, Acc: 0.6661\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 6/25 | Train Loss: 0.3876, Acc: 0.6971 | Val Loss: 0.8438, Acc: 0.6702\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 7/25 | Train Loss: 0.3773, Acc: 0.7090 | Val Loss: 0.8305, Acc: 0.6649\n",
      "Epoch 8/25 | Train Loss: 0.3668, Acc: 0.7283 | Val Loss: 0.8286, Acc: 0.6809\n",
      "Epoch 9/25 | Train Loss: 0.3579, Acc: 0.7396 | Val Loss: 0.8417, Acc: 0.6673\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 10/25 | Train Loss: 0.3478, Acc: 0.7497 | Val Loss: 0.8430, Acc: 0.6738\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 11/25 | Train Loss: 0.3428, Acc: 0.7666 | Val Loss: 0.8392, Acc: 0.6762\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 12/25 | Train Loss: 0.3354, Acc: 0.7666 | Val Loss: 0.8655, Acc: 0.6744\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 13/25 | Train Loss: 0.3290, Acc: 0.7782 | Val Loss: 0.8483, Acc: 0.6839\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 14/25 | Train Loss: 0.3195, Acc: 0.7860 | Val Loss: 0.8599, Acc: 0.6791\n",
      "EarlyStopping counter: 6 / 10\n",
      "Epoch 15/25 | Train Loss: 0.3079, Acc: 0.7982 | Val Loss: 0.8654, Acc: 0.6791\n",
      "EarlyStopping counter: 7 / 10\n",
      "Epoch 16/25 | Train Loss: 0.3022, Acc: 0.8124 | Val Loss: 0.8696, Acc: 0.6786\n",
      "EarlyStopping counter: 8 / 10\n",
      "Epoch 17/25 | Train Loss: 0.2975, Acc: 0.8127 | Val Loss: 0.8713, Acc: 0.6863\n",
      "EarlyStopping counter: 9 / 10\n",
      "Epoch 18/25 | Train Loss: 0.2898, Acc: 0.8267 | Val Loss: 0.8785, Acc: 0.6910\n",
      "EarlyStopping counter: 10 / 10\n",
      "Early stopping triggered at epoch 18. Best Val Acc: 0.6910\n",
      "\n",
      "Training completed in 0h 23m\n",
      "Best Val Acc: 0.6910 at epoch 18\n",
      "\n",
      "=== Fold 2/3 ===\n",
      "Epoch 1/25 | Train Loss: 0.5119, Acc: 0.4921 | Val Loss: 0.9284, Acc: 0.5993\n",
      "Epoch 2/25 | Train Loss: 0.4593, Acc: 0.5976 | Val Loss: 0.8903, Acc: 0.6076\n",
      "Epoch 3/25 | Train Loss: 0.4342, Acc: 0.6348 | Val Loss: 0.8644, Acc: 0.6439\n",
      "Epoch 4/25 | Train Loss: 0.4144, Acc: 0.6571 | Val Loss: 0.8524, Acc: 0.6463\n",
      "Epoch 5/25 | Train Loss: 0.3994, Acc: 0.6782 | Val Loss: 0.8639, Acc: 0.6445\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 6/25 | Train Loss: 0.3894, Acc: 0.6918 | Val Loss: 0.8440, Acc: 0.6671\n",
      "Epoch 7/25 | Train Loss: 0.3783, Acc: 0.7064 | Val Loss: 0.8516, Acc: 0.6659\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 8/25 | Train Loss: 0.3704, Acc: 0.7263 | Val Loss: 0.8706, Acc: 0.6635\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 9/25 | Train Loss: 0.3579, Acc: 0.7385 | Val Loss: 0.8478, Acc: 0.6724\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 10/25 | Train Loss: 0.3498, Acc: 0.7447 | Val Loss: 0.8532, Acc: 0.6712\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 11/25 | Train Loss: 0.3406, Acc: 0.7623 | Val Loss: 0.8780, Acc: 0.6659\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 12/25 | Train Loss: 0.3346, Acc: 0.7673 | Val Loss: 0.8624, Acc: 0.6742\n",
      "EarlyStopping counter: 6 / 10\n",
      "Epoch 13/25 | Train Loss: 0.3224, Acc: 0.7816 | Val Loss: 0.8698, Acc: 0.6730\n",
      "EarlyStopping counter: 7 / 10\n",
      "Epoch 14/25 | Train Loss: 0.3191, Acc: 0.7851 | Val Loss: 0.8717, Acc: 0.6790\n",
      "EarlyStopping counter: 8 / 10\n",
      "Epoch 15/25 | Train Loss: 0.3114, Acc: 0.8003 | Val Loss: 0.8749, Acc: 0.6712\n",
      "EarlyStopping counter: 9 / 10\n",
      "Epoch 16/25 | Train Loss: 0.3097, Acc: 0.7988 | Val Loss: 0.8829, Acc: 0.6819\n",
      "EarlyStopping counter: 10 / 10\n",
      "Early stopping triggered at epoch 16. Best Val Acc: 0.6819\n",
      "\n",
      "Training completed in 0h 20m\n",
      "Best Val Acc: 0.6819 at epoch 16\n",
      "\n",
      "=== Fold 3/3 ===\n",
      "Epoch 1/25 | Train Loss: 0.5037, Acc: 0.5019 | Val Loss: 0.9396, Acc: 0.5874\n",
      "Epoch 2/25 | Train Loss: 0.4548, Acc: 0.5932 | Val Loss: 0.8673, Acc: 0.6463\n",
      "Epoch 3/25 | Train Loss: 0.4288, Acc: 0.6392 | Val Loss: 0.8518, Acc: 0.6320\n",
      "Epoch 4/25 | Train Loss: 0.4082, Acc: 0.6582 | Val Loss: 0.8566, Acc: 0.6350\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 5/25 | Train Loss: 0.3919, Acc: 0.6921 | Val Loss: 0.8443, Acc: 0.6641\n",
      "Epoch 6/25 | Train Loss: 0.3768, Acc: 0.7064 | Val Loss: 0.8485, Acc: 0.6712\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 7/25 | Train Loss: 0.3701, Acc: 0.7153 | Val Loss: 0.8426, Acc: 0.6795\n",
      "Epoch 8/25 | Train Loss: 0.3626, Acc: 0.7278 | Val Loss: 0.8486, Acc: 0.6712\n",
      "EarlyStopping counter: 1 / 10\n",
      "Epoch 9/25 | Train Loss: 0.3539, Acc: 0.7397 | Val Loss: 0.8689, Acc: 0.6593\n",
      "EarlyStopping counter: 2 / 10\n",
      "Epoch 10/25 | Train Loss: 0.3462, Acc: 0.7450 | Val Loss: 0.8656, Acc: 0.6599\n",
      "EarlyStopping counter: 3 / 10\n",
      "Epoch 11/25 | Train Loss: 0.3388, Acc: 0.7578 | Val Loss: 0.8821, Acc: 0.6718\n",
      "EarlyStopping counter: 4 / 10\n",
      "Epoch 12/25 | Train Loss: 0.3271, Acc: 0.7724 | Val Loss: 0.8900, Acc: 0.6760\n",
      "EarlyStopping counter: 5 / 10\n",
      "Epoch 13/25 | Train Loss: 0.3235, Acc: 0.7747 | Val Loss: 0.9099, Acc: 0.6861\n",
      "EarlyStopping counter: 6 / 10\n",
      "Epoch 14/25 | Train Loss: 0.3162, Acc: 0.7920 | Val Loss: 0.9052, Acc: 0.6766\n",
      "EarlyStopping counter: 7 / 10\n",
      "Epoch 15/25 | Train Loss: 0.3076, Acc: 0.7979 | Val Loss: 0.8981, Acc: 0.6873\n",
      "EarlyStopping counter: 8 / 10\n",
      "Epoch 16/25 | Train Loss: 0.3013, Acc: 0.8092 | Val Loss: 0.8926, Acc: 0.6837\n",
      "EarlyStopping counter: 9 / 10\n",
      "Epoch 17/25 | Train Loss: 0.2996, Acc: 0.8074 | Val Loss: 0.8912, Acc: 0.6825\n",
      "EarlyStopping counter: 10 / 10\n",
      "Early stopping triggered at epoch 17. Best Val Acc: 0.6873\n",
      "\n",
      "Training completed in 0h 21m\n",
      "Best Val Acc: 0.6873 at epoch 15\n",
      "\n",
      "Ensemble trained with 3 folds.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "# ====================== EARLY STOPPING ======================\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping utility to stop training when a monitored metric stops improving.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    patience : int\n",
    "        Number of epochs to wait after last improvement before stopping.\n",
    "    delta : float\n",
    "        Minimum change to qualify as improvement.\n",
    "    verbose : bool\n",
    "        Whether to print messages when counter increases.\n",
    "    mode : str\n",
    "        'min' for metrics to minimize (e.g., loss), 'max' for metrics to maximize (e.g., accuracy).\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, delta=0.0, verbose=False, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, metric_value):\n",
    "        \"\"\"\n",
    "        Call method to update early stopping status.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        metric_value : float\n",
    "            Current metric value to monitor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if training should stop, False otherwise.\n",
    "        \"\"\"\n",
    "        score = -metric_value if self.mode == 'min' else metric_value\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        if score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "# ====================== TRAINER ======================\n",
    "class MultimodalTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for multimodal model with gradient accumulation, max gradient clipping,\n",
    "    and optional learning rate scheduler.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        PyTorch model to train.\n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer instance.\n",
    "    criterion : nn.Module\n",
    "        Loss function.\n",
    "    scheduler : torch.optim.lr_scheduler._LRScheduler, optional\n",
    "        Learning rate scheduler.\n",
    "    grad_accum_steps : int\n",
    "        Number of steps for gradient accumulation.\n",
    "    max_grad_norm : float\n",
    "        Maximum gradient norm for clipping.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device, optimizer, criterion, scheduler=None, grad_accum_steps=2, max_grad_norm=1.0):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.grad_accum_steps = grad_accum_steps\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.best_metrics = {'val_loss': float('inf'), 'val_acc': 0, 'epoch': 0}\n",
    "\n",
    "    def _to_device(self, batch):\n",
    "        \"\"\"Move batch dictionary to device.\"\"\"\n",
    "        return {k: v.to(self.device) for k, v in batch.items()}\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_loader : DataLoader\n",
    "            Training dataloader.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Average training loss.\n",
    "        float\n",
    "            Training accuracy.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch = self._to_device(batch)\n",
    "            outputs = self.model(\n",
    "                batch['video_feat'], batch['text_feat'], batch['caption_feat'],\n",
    "                video_mask=batch.get('video_mask', None),\n",
    "                text_mask=batch.get('text_mask', None)\n",
    "            )\n",
    "            loss = self.criterion(outputs, batch['label']) / self.grad_accum_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % self.grad_accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * batch['label'].size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch['label']).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "        return total_loss / total, correct / total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"\n",
    "        Evaluate the model on validation data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        val_loader : DataLoader\n",
    "            Validation dataloader.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Average validation loss.\n",
    "        float\n",
    "            Validation accuracy.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch in val_loader:\n",
    "            batch = self._to_device(batch)\n",
    "            outputs = self.model(\n",
    "                batch['video_feat'], \n",
    "                batch['text_feat'], \n",
    "                batch['caption_feat'],\n",
    "                video_mask=batch.get('video_mask', None),\n",
    "                text_mask=batch.get('text_mask', None)\n",
    "            )\n",
    "            loss = self.criterion(outputs, batch['label'])\n",
    "            total_loss += loss.item() * batch['label'].size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch['label']).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "        return total_loss / total, correct / total\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs=25, patience=10, checkpoint_dir=\"checkpoints\"):\n",
    "        \"\"\"\n",
    "        Full training loop with early stopping and checkpoint saving.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_loader : DataLoader\n",
    "            Training dataloader.\n",
    "        val_loader : DataLoader\n",
    "            Validation dataloader.\n",
    "        num_epochs : int\n",
    "            Maximum number of training epochs.\n",
    "        patience : int\n",
    "            Early stopping patience.\n",
    "        checkpoint_dir : str\n",
    "            Directory to save checkpoints.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Best validation metrics including loss, accuracy, and epoch.\n",
    "        \"\"\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)  \n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "\n",
    "        start_time = time.time()\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True, mode='min')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            val_loss, val_acc = self.validate(val_loader)\n",
    "\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step(val_loss)\n",
    "\n",
    "            if val_acc > self.best_metrics['val_acc']:\n",
    "                self.best_metrics.update({'val_loss': val_loss, 'val_acc': val_acc, 'epoch': epoch + 1})\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc\n",
    "                }, checkpoint_path)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "            if early_stopping(val_loss):\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Acc: {self.best_metrics['val_acc']:.4f}\")\n",
    "                break\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {total_time//3600:.0f}h {(total_time%3600)//60:.0f}m\")\n",
    "        print(f\"Best Val Acc: {self.best_metrics['val_acc']:.4f} at epoch {self.best_metrics['epoch']}\")\n",
    "\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return self.best_metrics\n",
    "\n",
    "# ====================== 3-FOLD CV with mask support ======================\n",
    "def run_3fold_cv(data_list, labels, create_dataloader, num_classes=3, batch_size=16, num_epochs=25, patience=10):\n",
    "    \"\"\"\n",
    "    Run 3-fold cross-validation training for a multimodal model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_list : list\n",
    "        List of data dictionaries.\n",
    "    labels : np.ndarray or torch.Tensor\n",
    "        Array of class labels.\n",
    "    create_dataloader : callable\n",
    "        Function to create dataloaders from data and labels.\n",
    "    num_classes : int\n",
    "        Number of classes.\n",
    "    batch_size : int\n",
    "        Batch size.\n",
    "    num_epochs : int\n",
    "        Maximum number of training epochs.\n",
    "    patience : int\n",
    "        Early stopping patience.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of trained models for each fold.\n",
    "    callable\n",
    "        Ensemble prediction function for a batch.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fold_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(data_list, labels)):\n",
    "        print(f\"\\n=== Fold {fold+1}/3 ===\")\n",
    "\n",
    "        data_train = [data_list[i] for i in train_idx]\n",
    "        labels_train = labels[train_idx]\n",
    "        data_val = [data_list[i] for i in val_idx]\n",
    "        labels_val = labels[val_idx]\n",
    "\n",
    "        train_loader = create_dataloader(data_train, labels_train, batch_size=batch_size, shuffle=True, augment=True)\n",
    "        val_loader = create_dataloader(data_val, labels_val, batch_size=batch_size, shuffle=False, augment=False)\n",
    "\n",
    "        model = MultimodalModel(\n",
    "            video_dim=256,\n",
    "            text_dim=256,\n",
    "            caption_dim=256,\n",
    "            hidden_dim=256,\n",
    "            num_classes=num_classes,\n",
    "            r=6,\n",
    "            finetune_mode='coattn_plus_latefusion_part'\n",
    "        )\n",
    "\n",
    "        # Class weights\n",
    "        class_weights = torch.tensor([1.22, 0.90, 0.94], device=device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.08)\n",
    "\n",
    "        # Optimizer + scheduler\n",
    "        params_to_update = model.get_param_groups(lr_late=1.5e-4, lr_coattn=1.5e-4, lr_fc_ca1=5e-5, weight_decay=5e-4)\n",
    "        optimizer = torch.optim.AdamW(params_to_update)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "        checkpoint_dir = f\"/Users/jumita/Downloads/checkpoints_fold_{fold+1}\"\n",
    "\n",
    "        trainer = MultimodalTrainer(model=model, device=device, optimizer=optimizer,\n",
    "                                     criterion=criterion, scheduler=scheduler, grad_accum_steps=2, max_grad_norm=1.0)\n",
    "\n",
    "        trainer.train(train_loader, val_loader, num_epochs=num_epochs, patience=patience, checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_dir, 'best_model.pth'), map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        fold_models.append(model)\n",
    "\n",
    "    print(\"\\nEnsemble trained with 3 folds.\")\n",
    "\n",
    "    def ensemble_predict(batch):\n",
    "        \"\"\"\n",
    "        Make predictions using the ensemble of 3 trained fold models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : dict\n",
    "            Batch dictionary with 'video_feat', 'text_feat', 'caption_feat'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Predicted class indices.\n",
    "        \"\"\"\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for model in fold_models:\n",
    "                output = model(\n",
    "                    batch['video_feat'], batch['text_feat'], batch['caption_feat'],\n",
    "                    video_mask=batch.get('video_mask', None),\n",
    "                    text_mask=batch.get('text_mask', None)\n",
    "                )\n",
    "                probs.append(torch.softmax(output, dim=1))\n",
    "        avg_probs = torch.stack(probs).mean(dim=0)\n",
    "        return avg_probs.argmax(dim=1)\n",
    "\n",
    "    return fold_models, ensemble_predict\n",
    "\n",
    "\n",
    "best_model = run_3fold_cv(data_list, labels, create_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a61a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Test | Acc: 0.8099, Prec: 0.8165, Rec: 0.8099, F1: 0.8097\n",
      "Final Ensemble Results: 0.80990099009901 0.816537841950824 0.80990099009901 0.8096565291857968\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"peft\").setLevel(logging.ERROR)\n",
    "\n",
    "def test_ensemble(fold_models, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate an ensemble of trained models on a test dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fold_models : list of nn.Module\n",
    "        List of trained fold models.\n",
    "    test_loader : DataLoader\n",
    "        Dataloader for test dataset.\n",
    "    device : torch.device\n",
    "        Device to run inference on (CPU/GPU/MPS).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Accuracy, weighted precision, weighted recall, weighted F1-score.\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for model in fold_models:\n",
    "                model.eval()\n",
    "                output = model(batch['video_feat'], batch['text_feat'], batch['caption_feat'])\n",
    "                probs.append(torch.softmax(output, dim=1))\n",
    "        avg_probs = torch.stack(probs).mean(dim=0)\n",
    "        predicted = avg_probs.argmax(dim=1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='weighted')\n",
    "    rec = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"Ensemble Test | Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "\n",
    "# ================= DEVICE =================\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# ================= CHECKPOINTS =================\n",
    "model_paths = [\n",
    "    \"/Users/jumita/Downloads/checkpoints_fold_1/best_model.pth\",\n",
    "    \"/Users/jumita/Downloads/checkpoints_fold_2/best_model.pth\",\n",
    "    \"/Users/jumita/Downloads/checkpoints_fold_3/best_model.pth\"\n",
    "]\n",
    "\n",
    "# ================= MODEL FACTORY =================\n",
    "def create_multimodal_model():\n",
    "    \"\"\"\n",
    "    Create a fresh instance of the MultimodalModel with the same architecture\n",
    "    as used during training.\n",
    "    \"\"\"\n",
    "    return MultimodalModel(\n",
    "        video_dim=256,\n",
    "        text_dim=256,\n",
    "        caption_dim=256,\n",
    "        hidden_dim=256,\n",
    "        num_classes=3,\n",
    "        r=6,  \n",
    "        finetune_mode='coattn_plus_latefusion_part'\n",
    "    )\n",
    "\n",
    "# ================= LOAD MODELS =================\n",
    "fold_models = []\n",
    "for path in model_paths:\n",
    "    checkpoint = torch.load(path, map_location=device)  # load checkpoint \n",
    "    model = create_multimodal_model()\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])  # load weight\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    fold_models.append(model)\n",
    "\n",
    "# ================= ENSEMBLE TEST =================\n",
    "acc, prec, rec, f1 = test_ensemble(fold_models, test_loader, device)\n",
    "print(\"Final Ensemble Results:\", acc, prec, rec, f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd-webui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
